{
  "hash": "445790b4aca8842157ca8eb74348c506",
  "result": {
    "markdown": "---\ntitle: Trimming Outliers for A/B Testing\ndate: '2022-12-08'\ncategories:\n  - Julia\nimage: ../../_freeze/posts/outliers-for-ab-testing/index/figure-html/fig-samples-output-1.svg\nbibliography: references.bib\ncap-location: margin\n---\n\n\n\n* Running A/B tests targeting continuous metrics like revenue per visitor are important for companies to evaluate the financial impact of their interventions\n* These metrics are often hard from a statistical perspective because they are influenced by outliers, which dramatically hurt experimental power\n* One way to think about these outliers is to say you don't want to optimize your website for a minority of extremely high value customers, since this is high risk. This thinking goes towards the ideas of quantitative finance, and I won't explore this in this post.\n* Another reason could be data issues. Lets assume we rule that out.\n* The second way is to say you want to optimize your website for everyone, so you can't ignore the outliers, but their existance makes standard statistical tests inefficient, so we try to find more efficient estimators that require introducing as little bias as possible.\n* We can formalise this by assuming that the majority of our sample comes from some well behaved population distribution, but a small number of samples are drawn from a population distribution with infinite variance. For example, the Pareto distribution [@powerlaws].\n* This contribution from the infinite variance distribution is a problem, because it causes the mixture distribution to also have infinite variance, and therefore the central limit theorem breaks down. \n\nTODO:\n* Compare trimming, winzoring and pareto smoothing with different cutoffs\n* Use KDE + Pareto mixture. Then take the mean of the MLE pareto + mean of other data for estimator. Use laplace approx for variance.\n* Basic check: is Hill better at estimating mean than sample mean?\n\nSECTIONS:\n1. Introduction on A/B testing, revenue as a metric, and outliers. Talk about outliers as coming from a distribution with infinite population variance.\n2. Infinite population variance: the Pareto distribution. What does infinite variance mean? Why is it a problem for A/B testing? Do a simulation with a simple welches t-test.\n3. What can we not do? Log transform, median test, non-parametric test. We care about the mean. \n4. Simple solutions: trimming vs windsorizing. These are a bias variance tradeoff. How do they perform? Do the same simulations.\n5. What if we model the tail as a pareto distribution directly? Try PSIS and a KDE + Pareto Mixture for MLE. \n6. Choosing the best cutoff? KS test, AIC, BIC, Mixture? Mixture doesn't need a cutoff - that's nice. \n7. Final comparison and results. My recommendation. Key points - choose the cutoff without looking at the results to avoid peaking.  \n\nMethod:\n\nThe following is inspired by the Pareto Smoothed Importance Sampling paper [@psis]:\n\n1. Set $M = min(0.2 S, 3 \\sqrt{S})$ OR find best M using KS-test [@powerlawsempirical];\n2. Set $\\hat{u} = x_{S-M}$\n3. Estimate $\\alpha$ of the tail, using the standard Hill estimator\n4. Set $x'_{S-M+z} = \\min{ \\left( F^{-1} \\left( \\frac{z - 1/2}{M} \\right), \\max_s{(x_s)} \\right)}$\n\nNow perform a t-test on $x'$ instead of $x$. \n\nHill estimator:\n\n$$\n\\alpha = 1 + n \\left[\\sum_{i=1}^n \\ln{ \\frac{x_i}{x_\\text{min}} } \\right]^{-1}\n$$\n\nThroughout this article we will use two types of distributions to represent distributions with infinite variance, the Pareto distribution and the Generalised Pareto Distribution (GPD). \n\n::: {.cell execution_count=2}\n\n::: {.cell-output .cell-output-display execution_count=3}\n![A comparison between the standard pareto distribution (red), the Generalised Pareto Distribution (blue), and the log-normal distribution (green). ](index_files/figure-html/fig-pareto-output-1.png){#fig-pareto}\n:::\n:::\n\n\nBoth of these distributions have a power law tail, and therefore can have infinite variance. This is noticable when plotted on a log-log scale (as is shown in @fig-pareto), where the tail will be linear. This is shown in contrast to a Log-Normal distribution, where the tail is not Linear, and eventually at very high $X$ the PDF drops below the power law tails. \n\nWhile the two types of pareto distribution have the same tail behavior, they differ in how they behave at small values of $X$. The Pareto distribution is a pure power law until some value, below which the probability mass is 0, while the Generalised Pareto Distribution smoothly transitions away from being power law at low X. The GPD is a more general distribution - with the correct parameters in the GPD you can get back to the Pareto distribution - however it can be harder to fit to data because it has more parameters. \n\nThe challenge for A/B testing can be demonstrated by comparing these distributions with the log-normal distribution. While you are always able to calculate the sample variance of any sample of data, whether it is from a population with finite or infinite variance, if the sample is drawn from a population with infinite variance then the sample variance will not converge as the sample size increases. You can see this in the example below, where I sample from a GPD and a LogNormal Distribution and then calculate the sample variance vs sample size. \n\n::: {.cell execution_count=3}\n\n::: {.cell-output .cell-output-display execution_count=4}\n![](index_files/figure-html/fig-inf-variance-output-1.png){#fig-inf-variance}\n:::\n:::\n\n\nThis causes problems for the t-test, since it uses the sample variance to estimate the standard error on the mean. So if your data has a power law tail, it's unlikely that this will give a good estimate. \n\nPower law tails are usualy quantified using the Hill Estimator. We can get the standard error on that estimate too. Let's compare the Hill estimator + Delta method with the mean and variance estimator for the regular Pareto distribution.\n\n::: {.cell execution_count=4}\n\n::: {.cell-output .cell-output-stdout}\n```\nSample Mean / Var Estimator\n(n=200) \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n-1.3965659791961105 1.9111371877365484\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nHill Estimator\n(n=200) -0.28134915336893007 1.0589227061388358\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nSample Mean / Var Estimator\n(n=10,000) -1.0553330056952266 1.5698344731984775\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nHill Estimator\n(n=10,000) -0.03310762675581043 0.9803369870820887\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=5}\n![Estimating the mean and the standard error for samples taken from a Pareto Distribution. Using the sample mean and variance (left) and using the Hill estimator (right), for two sample sizes: 200 (top) and 10000 (bottom). Compared with the expected t-distribtuion with d.o.f = samples size - 1.](index_files/figure-html/fig-hill-estimator-output-6.png){#fig-hill-estimator}\n:::\n:::\n\n\nYou can see that the hill estimator performs much better than using the same mean and variance. \n\nSo we want to use the Hill estimator instead of the sample mean and variance, but **only in the tail**. The rest of the distribution is unlikely to be pareto distributed, so the Hill estimator would fail there. This brings us to the next challenge: how do we decide when we are \"in the tail\"?\n\nTo do this, we will use a semi-parametric mixture of the Pareto distribution in the tail and a Kernel Density Approximation (KDE) elsewhere. The KDE bandwidth and cutoff threshold for the tail will be chosen by maximizing the Leave-One-Out (LOO) likelihood.\n\nTo test this out, I'll create a synthetic dataset which is a mixture of a GPD and two lognormal distributions. This will make the main distribution bimodal (and so not too easy for our method) but with a power law tail that gives it infinite variance. This distribution is shown in @fig-samples. \n\n::: {.cell execution_count=5}\n\n::: {.cell-output .cell-output-display execution_count=6}\n![Histogram of samples from simulated data (blue), and the theoretical PDF (orange).](index_files/figure-html/fig-samples-output-1.png){#fig-samples}\n:::\n:::\n\n\nNow I can build my method to estimate the mean of that distribution, and the standard error on the mean. \n\nNow lets investigate the tail of this distribution. Does it look like a power law? Can we fit one to it?\n\n1. Set the tail threshold as the smallest possible value where the KS test p_value >= 0.05\n1. Calculate mean and standard error using Hill estimator for all samples in the tail\n2. Calculate normal mean and standard error for all samples, also based on their weights\n3. Combine the two estimates to get an overal mean and standard error\n\nITS CLOSE BUT I MUST HAVE SLIGHTLY THE WRONG FORMULAS\n\n::: {.cell execution_count=6}\n\n::: {.cell-output .cell-output-stdout}\n```\nSample Mean / Var Estimator -1.019205622066096 1.5717106001048053\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nTrimmed Estimator -53.63337250341208 1.6880891161091203\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nWindsorized Estimator -35.66970540022096 1.7011737548929817\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nHill Tail Estimator -0.23894055319894975 1.1651315741813204\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=7}\n![](index_files/figure-html/cell-7-output-5.png){}\n:::\n:::\n\n\n\n\n\n\nNow I will implement my method\n\n\n\nNow test it systematically\n\n\n\n\n\n# OLD STUFF\n\nIn a lot of situations we have 1-D data which is skewed, and we want to remove the outliers on the right. This happens a lot when running experiments with continuous variables like revenue, where you can have a few data points which are very high revenue and hurt your variance a lot. Removing these outliers improves your variance a lot.\n\n## Generate toy dataset\n\nTo test these methods, I need some synthetic data. I'll generate a mixture of a log-normal distribution and a Pareto distribution. The lognormal represents most users, but the Pareto represents the outliers. The pareto is especially problematic for statistics, since it has infinite variance, meaning the CLT won't work.\n\n\n\n## Outlier Detection\n\nNow we want to try to identify the outliers that came from the pareto distributon. I'll try to do this by modelling the data as:\n\n1. A KDE density estimation. This is for the bulk data, and is non-parametric so it should work on any dataset. I need to choose a kernel, for this data I'll choose lognormal.\n2. A generalized pareto distribution. This should model the outliers.\n\n\n\n\n\n\n\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}