{
  "hash": "6ea9236d1af252c0c916aae74d3b64c9",
  "result": {
    "markdown": "---\ntitle: Trimming Outliers for A/B Testing\ndate: '2022-12-08'\ncategories:\n  - Julia\nimage: ../../_freeze/posts/outliers-for-ab-testing/index/figure-html/cell-3-output-1.svg\nbibliography: references.bib\n---\n\n\n\n* Running A/B tests targeting continuous metrics like revenue per visitor are important for companies to evaluate the financial impact of their interventions\n* These metrics are often hard from a statistical perspective because they are influenced by outliers, which dramatically hurt experimental power\n* One way to think about these outliers is to say you don't want to optimize your website for a minority of extremely high value customers, since this is high risk. This thinking goes towards the ideas of quantitative finance, and I won't explore this in this post.\n* Another reason could be data issues. Lets assume we rule that out.\n* The second way is to say you want to optimize your website for everyone, so you can't ignore the outliers, but their existance makes standard statistical tests inefficient, so we try to find more efficient estimators that require introducing as little bias as possible.\n* We can formalise this by assuming that the majority of our sample comes from some well behaved population distribution, but a small number of samples are drawn from a population distribution with infinite variance. For example, the Pareto distribution [@powerlaws].\n* This contribution from the infinite variance distribution is a problem, because it causes the mixture distribution to also have infinite variance, and therefore the central limit theorem breaks down. \n\n\nMethod:\n\nThe following is inspired by the Pareto Smoothed Importance Sampling paper [@psis]:\n\n1. Set $M = min(0.2 S, 3 \\sqrt{S})$\n2. Set $\\hat{u} = x_{S-M}$\n3. Estimate $\\alpha$ of the tail, using the standard Hill estimator\n4. Set $x'_{S-M+z} = \\min{ \\left( F^{-1} \\left( \\frac{z - 1/2}{M} \\right), \\max_s{(x_s)} \\right)}$\n\nNow perform a t-test on $x'$ instead of $x$. \n\nHill estimator:\n\n$$\n\\alpha = 1 + n \\left[\\sum_{i=1}^n \\ln{ \\frac{x_i}{x_\\text{min}} } \\right]^{-1}\n$$\n\n::: {.cell execution_count=2}\n\n::: {.cell-output .cell-output-display execution_count=3}\n![](index_files/figure-html/cell-3-output-1.svg){}\n:::\n:::\n\n\nNow lets investigate the tail of this distribution. Does it look like a power law? Can we fit one to it?\n\n::: {.cell execution_count=3}\n\n::: {.cell-output .cell-output-stdout}\n```\n4.518809851321794\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=4}\n![](index_files/figure-html/cell-4-output-2.svg){}\n:::\n:::\n\n\nNow I will implement my method\n\n::: {.cell execution_count=4}\n\n::: {.cell-output .cell-output-stdout}\n```\n4.518809851321794\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=5}\n![](index_files/figure-html/cell-5-output-2.svg){}\n:::\n:::\n\n\n# OLD STUFF\n\nIn a lot of situations we have 1-D data which is skewed, and we want to remove the outliers on the right. This happens a lot when running experiments with continuous variables like revenue, where you can have a few data points which are very high revenue and hurt your variance a lot. Removing these outliers improves your variance a lot.\n\n## Generate toy dataset\n\nTo test these methods, I need some synthetic data. I'll generate a mixture of a log-normal distribution and a Pareto distribution. The lognormal represents most users, but the Pareto represents the outliers. The pareto is especially problematic for statistics, since it has infinite variance, meaning the CLT won't work.\n\n::: {.cell execution_count=5}\n\n::: {.cell-output .cell-output-display execution_count=6}\n![](index_files/figure-html/cell-6-output-1.svg){}\n:::\n:::\n\n\n## Outlier Detection\n\nNow we want to try to identify the outliers that came from the pareto distributon. I'll try to do this by modelling the data as:\n\n1. A KDE density estimation. This is for the bulk data, and is non-parametric so it should work on any dataset. I need to choose a kernel, for this data I'll choose lognormal.\n2. A generalized pareto distribution. This should model the outliers.\n\n::: {.cell execution_count=6}\n\n::: {.cell-output .cell-output-display execution_count=7}\n![](index_files/figure-html/cell-7-output-1.svg){}\n:::\n:::\n\n\n::: {.cell execution_count=7}\n\n::: {.cell-output .cell-output-display execution_count=8}\n![](index_files/figure-html/cell-8-output-1.svg){}\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}