{
  "hash": "655b55951e8a7c5d55e4eedadbfe10d2",
  "result": {
    "markdown": "---\ntitle: The statistics of relative effect sizes in A/B testing.\ndate: '2023-06-25'\ncategories:\n  - Python\nimage: ../../_freeze/posts/models_vs_markets/index/figure-html/cell-2-output-1.png\ndescription: What's the best way to handle relative effects in A/B testing?\ndraft: true\nhtml-math-method: webtex\n---\n\n# Introduction\n\nRatio metrics are critical when analyzing online controlled trials.\n\n# The likelihood ratio\n\nThe test statistic for the likelihood ratio test is: \n\n$$\n\\lambda_{\\mathrm{LR}}=-2\\left[l\\left(\\theta_0\\right)-l(\\hat{\\theta})\\right]\n$$\n\nWhere $l\\left(\\theta\\right)$  is the log-likelihood of the observed data, $\\theta$ are the parameters of the model, $\\theta_0$ is the parameters under the null hypothesis, and $\\hat{\\theta}$ are the maximum likelihood estimates of the parameters. The model for $\\theta_0$ must be a subset of the model for $\\hat{\\theta}$, itâ€™s normally the same model but with 1 or more of the parameters fixed at a specific value.\n\nThe sampling distribution for $\\lambda_{\\mathrm{LR}}$ is often not know exactly, but if the sample size is large enough it should be approximately $\\chi^2$ distributed, with the degrees of freedom equal to the difference in the number of free parameters in the two models. \n\nLet's see if we can use this to first derive the two sample welches t-test, which tests the absolute difference between the means of two samples. Then if we can do that, we can apply the same approach to create a test for the relative difference.\n\n# The likelihood ratio for the means of two independent samples of data\n\nAssume we have two samples of data, $\\mathbf{x}$ and $\\mathbf{y}$, with $n_x$ and $n_y$ samples respectively. I will model the two samples of data with two independent normal distributions. Therefore, I can write the joint likelihood of the two samples of data as:\n\n$$\nL(\\mathbf{x}, \\mathbf{y} | \\theta) = \\prod_{i=1}^{n_x} N(x_i | \\mu_x, \\sigma_x) \\prod_{j=1}^{n_y} N(y_j | \\mu_y, \\sigma_y)\n$$\n\nWhere $\\theta = [\\mu_x, \\mu_y, \\sigma_x, \\sigma_y]$. And the log-likelihood as:\n\n$$\nl(\\mathbf{x}, \\mathbf{y} | \\theta) = l(\\theta) = \\sum_{i=1}^{n_x} \\ln \\left( N(x_i | \\mu_x, \\sigma_x) \\right) + \\sum_{j=1}^{n_y} \\ln \\left( N(y_j | \\mu_y, \\sigma_y) \\right)\n$$\n\nThe log-likelihood of the normal distribution is:\n\n$$\n\\ln \\left( N(x | \\mu, \\sigma) \\right) = - \\frac{1}{2} \\ln{2 \\pi} - \\ln{\\sigma} - \\frac{1}{2 \\sigma^2}\\left( x - \\mu \\right)^2\n$$\n\nTherefore, we can write the log-likelihood of our data as:\n\n$$\nl(\\theta) = - \\left( \\frac{n_x + n_y}{2} \\ln{2 \\pi} + n_x \\ln{\\sigma_x} + n_y \\ln{\\sigma_y} + \\frac{1}{2 \\sigma_x^2} \\sum_{i=1}^{n_x}  \\left( x - \\mu_x \\right)^2 + \\frac{1}{2 \\sigma_y^2} \\sum_{i=1}^{n_y}  \\left( y - \\mu_y \\right)^2 \\right)\n$$\n\nNow we don't care about the the standard deviations, our null hypotheses are always about the means. So let's maximise the log-likelihood with respect to $\\sigma_x$ and $\\sigma_y$.\n\n$$\n0 = \\frac{\\partial l}{\\partial \\sigma_x} = - \\frac{n_x}{\\sigma_x} + \\frac{1}{\\sigma_x^3} \\sum_{i=1}^{n_x}  \\left( x - \\mu_x \\right)^2\n$$\n\nSolving this gives (and the same for $\\sigma_y$):\n\n$$\n\\sigma_x^2 = \\frac{1}{n_x} \\sum_{i=1}^{n_x}  \\left( x - \\mu_x \\right)^2\n$$\n\nSubstituting this back into the likelihood function gives:\n\n$$\nl(\\theta) = - \\left( \\frac{n_x + n_y}{2}  (\\ln{2 \\pi} + 1) + \\frac{n_x}{2} \\ln \\left( \\frac{1}{n_x} \\sum_{i=1}^{n_x}  \\left( x - \\mu_x \\right)^2 \\right) + \\frac{n_y}{2} \\ln \\left( \\frac{1}{n_y} \\sum_{i=1}^{n_y}  \\left( y - \\mu_y \\right)^2 \\right) \\right)\n$$\n\nNow to estimate $l(\\hat{\\theta})$ I need to maximize with respect to $\\mu_x$ and $\\mu_y$ too. While I could do this again formally by differentiating $l(\\theta)$, it's not really necessary. It's clear that the maximum is when $\\mu_x = \\bar{x}$, where $\\bar{x}$ is the mean of $\\mathbf{x}$, and the same for $y$. This gives:\n\n$$\nl(\\hat{\\theta}) = - \\left( \\frac{n_x + n_y}{2}  (\\ln{2 \\pi} + 1) + \\frac{n_x}{2} \\ln \\hat{\\sigma}_x^2 + \\frac{n_y}{2} \\ln \\hat{\\sigma}_y^2 \\right)\n$$\n\nWhere $\\hat{\\sigma}_x$ and $\\hat{\\sigma}_y$ are the uncorrected sample standard deviations of $\\mathbf{x}$ and $\\mathbf{y}$. Now using these two expressions, I can calculate $\\lambda_{\\mathrm{LR}}$ for the general case under the null hypothesis that the population mean of $\\mathbf{x} = \\mu_x$ and the population mean of $\\mathbf{y} = \\mu_y$:\n\n$$\n\\lambda_{\\mathrm{LR}}(\\mu_x, \\mu_y) = n_x \\ln \\left( \\frac{1}{n_x \\hat{\\sigma}_x^2} \\sum_{i=1}^{n_x}  \\left( x - \\mu_x \\right)^2 \\right) + n_y \\ln \\left( \\frac{1}{n_y \\hat{\\sigma}_y^2} \\sum_{i=1}^{n_y}  \\left( y - \\mu_y \\right)^2 \\right)\n$$\n\nNow I can use a trick to simplify this further, by noting that:\n\n\\begin{align}\n\\frac{1}{n_x \\hat{\\sigma}_x^2} \\sum_{i=1}^{n_x}  \\left( x - \\mu_x \\right)^2 &= \\frac{1}{n_x \\hat{\\sigma}_x^2} \\sum_{i=1}^{n_x}  \\left( x - \\bar{x} + \\bar{x} - \\mu_x \\right)^2 \\nonumber \\\\\n&= \\frac{1}{n_x \\hat{\\sigma}_x^2} \\sum_{i=1}^{n_x}  \\left( \\left( x - \\bar{x} \\right)^2 + \\left( \\bar{x} - \\mu_x \\right)^2 + 2 (x - \\bar{x})(\\bar{x} - \\mu_x) \\right) \\nonumber \\\\\n&= \\frac{1}{n_x \\hat{\\sigma}_x^2} \\sum_{i=1}^{n_x}  \\left( \\left( x - \\bar{x} \\right)^2 + \\left( \\bar{x} - \\mu_x \\right)^2 \\right) \\nonumber \\\\\n&= 1 + \\frac{\\left( \\bar{x} - \\mu_x \\right)^2}{\\hat{\\sigma}_x^2} \\nonumber\n\\end{align}\n\nFinally substituting that back into the original equation gives:\n\n$$\n\\lambda_{\\mathrm{LR}}(\\mu_x, \\mu_y) =n_x \\ln \\left( 1 + \\frac{(\\bar{x} - \\mu_x)^2}{\\hat{\\sigma}_x^2} \\right) + n_y \\ln \\left( 1 + \\frac{(\\bar{y} - \\mu_y)^2}{\\hat{\\sigma}_y^2} \\right)\n$$\n\nThis can be approximated using the taylor expansion:\n\n$$\n\\ln(1+x^2) \\approx x^2 + O(x^4)\n$$\n\nUsing this gives:\n\n$$\n\\lambda_{\\mathrm{LR}}(\\mu_x, \\mu_y) =\\frac{(\\bar{x} - \\mu_x)^2}{\\hat{\\sigma}_x^2 / n_x} + \\frac{(\\bar{y} - \\mu_y)^2}{\\hat{\\sigma}_y^2 / n_y}\n$$\n\nThis looks like the sum of two t-distributed random variables squared. I will simplify this further by defining the standard error as, giving:\n\n$$\n\\lambda_{\\mathrm{LR}}(\\mu_x, \\mu_y) = \\frac{(\\bar{x} - \\mu_x)^2}{\\sigma^2_{\\mu, x}}\n$$\n\n\n\n# Deriving Welch's t-test from the likelihood ratio\n\nThe Welch's t-test tests the null hypothesis that the absolute difference between two means is some fixed value:\n\n\\begin{align}\nH_0 &: \\quad \\mu_y - \\mu_x = \\Delta \\nonumber \\\\\nH_A &: \\quad \\mu_y - \\mu_x \\neq \\Delta\\nonumber\n\\end{align}\n\nTo get the test statistic for this, I need to minimize $\\lambda_{\\mathrm{LR}}$ under the constraint that $\\mu_y - \\mu_x = \\Delta$. To do this I'll substitute $\\mu_y = \\mu_x + \\Delta$ into the equation for $\\lambda_{\\mathrm{LR}}$.\n\n\\begin{align}\n\\lambda_{\\mathrm{LR}}(\\mu_x, \\Delta) &= \\frac{(\\bar{x} - \\mu_x)^2}{\\sigma_\\bar{x}^2}+ \\frac{(\\bar{y} - \\mu_x - \\Delta )^2}{\\sigma_\\bar{y}^2} \\nonumber \\\\\n&= \\frac{(\\mu_x - \\bar{x})^2}{\\sigma_\\bar{x}^2}+ \\frac{(\\mu_x - (\\bar{y} - \\Delta) )^2}{\\sigma_\\bar{y}^2} \\nonumber\n\\end{align}\n\nNow it can be shown that in general:\n\n$$\n\\min_x \\left\\{ \\frac{(x - b)^2}{a^2} + \\frac{(x - d)^2}{c^2} \\right\\} = \\frac{(b - d)^2}{a^2 + c^2}\n$$\n\nI can use this to find the minimum of $\\lambda_{\\mathrm{LR}}$ without doing lots of differentiation. It gives:\n\n$$\n\\lambda_{\\mathrm{LR}}(\\Delta) = \\frac{\\left( \\bar{y} - \\bar{x} - \\Delta \\right)^2}{\\sigma_\\bar{x}^2 + \\sigma_\\bar{y}^2}\n$$\n\nFinally, taking the square root of this, gives the Welch's t-test statistic:\n\n$$\nt\n$$\n\n\n\nSo we have successfully derived the Welch's t-test from the likelihood ratio!\n\n# Deriving a statistical test for the ratio of means\n\nNow, given that this works for the regular difference between two means, let's see what we get for a ratio! This is a different null hypothesis:\n\n\\begin{align}\nH_0 &: \\quad \\frac{\\mu_y}{\\mu_x} = \\delta \\nonumber \\\\\nH_A &: \\quad  \\frac{\\mu_y}{\\mu_x} \\neq \\delta \\nonumber\n\\end{align}\n\nTaking the same approach as before, I substitute in that $\\mu_y = \\delta \\mu_x$:\n\n\\begin{align}\n\\lambda_{\\mathrm{LR}}(\\mu_x, \\delta) &= \\frac{(\\bar{x} - \\mu_x)^2}{\\sigma_\\bar{x}^2}+ \\frac{(\\bar{y} - \\mu_x \\delta )^2}{\\sigma_\\bar{y}^2} \\nonumber \\\\\n&= \\frac{(\\mu_x - \\bar{x})^2}{\\sigma_\\bar{x}^2}+ \\frac{(\\mu_x - \\frac{\\bar{y}}{\\delta}) )^2}{\\sigma_\\bar{y}^2 / \\delta^2} \\nonumber \\nonumber\n\\end{align}\n\nAnd using the same trick again I can find the minimum:\n\n$$\n\\lambda_{\\mathrm{LR}}(\\delta) = \\frac{\\left( \\bar{y} - \\delta \\bar{x} \\right)^2}{\\sigma_\\bar{y}^2 + \\delta^2 \\sigma_\\bar{x}^2 }\n$$\n\nAnd again taking the square root gives a t-distributed metric:\n\n$$\nt_\\delta(\\delta) = \\frac{\\bar{y} - \\delta \\bar{x} }{\\sqrt{\\sigma_\\bar{y}^2 + \\delta^2 \\sigma_\\bar{x}^2}}\n$$\n\nThis metric can be used as a statistical test for the ratio $\\delta$!\n\n## Comparison with Welch's t-test\n\nNow while this is t-distributed, it's actually different to the original Welch's t-statistic. I can check that by substituting the absolute difference in for the relative difference, $\\delta = \\frac{\\Delta}{\\bar{x}} + 1$. This gives:\n\n$$\nt_\\delta(\\Delta) = \\frac{ \\bar{y} -\\bar{x} - \\Delta }{\\sqrt{\\sigma_\\bar{y}^2 + \\left(\\frac{\\Delta}{\\bar{x}} + 1 \\right)^2 \\sigma_\\bar{x}^2 }} \\neq t_\\Delta(\\Delta)\n$$\n\nInterestingly the numerator is the same as in $t_\\Delta$, but the denominator is different! The variance depends on $\\Delta$. So we've found something new, it's not just rearranging Welch's t-test. \n\nNow while these are not equal in general, they are equal in the very important case - the case where you are testing the null hypothesis that $\\mu_x = \\mu_y$, giving $\\Delta = 0$ and $\\delta = 1$. In this case, the above formula reduces to:\n\n$$\nt_\\delta(0) = \\frac{ \\bar{y} -\\bar{x}}{\\sqrt{\\sigma_\\bar{y}^2 +  \\sigma_\\bar{x}^2 }} = t_\\Delta(0)\n$$\n\nThis is a very useful finding. If this were not the case, you could have different results for whether you can reject the null hypothesis that two samples have the same population mean, just depending on whether you care about reporting on the relative or absolute difference. It's of course sensible that the statistical tests should be different in general, but in this special case it's sensible that they coincide.\n\n# Confidence intervals\n\nWe have a statistical test now, but what about confidence intervals? Cox and Hinkly have a very nice definition of the confidence interval in their book, Theoretical Statistics. They say that the confidence interval contains all the values that are statistically consistent with the data.\n\nSo we can construct confidence intervals in the same way. They are all the values that the null hypothesis that the population has that value would not be rejected by our new statistical tests.\n\nFor a specific false positive rate, $\\alpha$, we reject the values that are more extreme than the critical t-values, $\\pm t_{1 - \\frac{\\alpha}{2}}$. Therefore, we have that:\n\n\\begin{align}\n\\mp t_{1 - \\frac{\\alpha}{2}} &= t_\\Delta(\\Delta_{\\pm}) \\nonumber \\\\\n\\mp t_{1 - \\frac{\\alpha}{2}} &= t_\\delta(\\delta_{\\pm}) \\nonumber\n\\end{align}\n\nNote that, above, the signs are opposite. This is because the highest value of $\\Delta$ or $\\delta$ causes the statistic to be lower. \n\n## Confidence interval for the absolute difference\n\nFirst the confidence interval for the absolute difference, $\\Delta_\\pm$. \n\n$$\n\\mp t_{1 - \\frac{\\alpha}{2}} = \\frac{\\bar{y} - \\bar{x} - \\Delta_{\\pm}}{\\sqrt{\\sigma_\\bar{x}^2 + \\sigma_\\bar{y}^2}}\n$$\n\nRe-arranging that gives the expected t-test confidence interval:\n\n$$\n\\Delta_{\\pm} =\\bar{y} - \\bar{x} \\pm t_{1-\\frac{\\alpha}{2}} \\sqrt{\\sigma_\\bar{x}^2 + \\sigma_\\bar{y}^2}\n$$\n\n## Confidence interval for the relative difference\n\nNow doing the same thing for the relative difference. We start with:\n\n$$\n\\mp t_{1 - \\frac{\\alpha}{2}} = \\frac{\\bar{y} - \\delta_\\pm \\bar{x} }{\\sqrt{\\sigma_\\bar{y}^2 + \\delta_\\pm^2 \\sigma_\\bar{x}^2}}\n$$\n\nThis is slightly trickier to re-arrange, but I'll give it a go:\n\n\\begin{align}\nt_{1 - \\frac{\\alpha}{2}}^2 &= \\frac{(\\bar{y} - \\delta_\\pm \\bar{x})^2}{\\sigma_\\bar{y}^2 + \\delta_\\pm^2 \\sigma_\\bar{x}^2} \\nonumber \\\\\n0 &= (\\bar{y} - \\delta_\\pm \\bar{x})^2 - t_{1 - \\frac{\\alpha}{2}}^2 \\left( \\sigma_\\bar{y}^2 + \\delta_\\pm^2 \\sigma_\\bar{x}^2 \\right) \\nonumber \\\\ \n0 &= \\bar{y}^2 + \\delta_\\pm^2 \\bar{x}^2 - 2 \\delta_\\pm \\bar{y} \\bar{x} - t_{1 - \\frac{\\alpha}{2}}^2 \\sigma_\\bar{y}^2 - t_{1 - \\frac{\\alpha}{2}}^2 \\delta_\\pm^2 \\sigma_\\bar{x}^2 \\nonumber \\\\ \n0 &= \\delta_\\pm^2 ( \\bar{x}^2 - \\sigma_\\bar{x}^2 t_{1 - \\frac{\\alpha}{2}}^2) - 2 \\delta_\\pm \\bar{y} \\bar{x} + \\bar{y}^2 - t_{1 - \\frac{\\alpha}{2}}^2 \\sigma_\\bar{y}^2  \\nonumber\n\\end{align}\n\nNow I can solve this quadratic equation, giving:\n\n$$\n\\delta_\\pm = \\frac{\\bar{y} \\bar{x} \\pm \\sqrt{\\bar{y}^2 \\bar{x}^2 - ( \\bar{x}^2 - \\sigma_\\bar{x}^2 t_{1 - \\frac{\\alpha}{2}}^2) (\\bar{y}^2 - t_{1 - \\frac{\\alpha}{2}}^2 \\sigma_\\bar{y}^2)}}{\\bar{x}^2 - \\sigma_\\bar{x}^2 t_{1 - \\frac{\\alpha}{2}}^2}\n$$\n\nThis is the same as the confidence interval from Fieller's method!\n\n# Conclusion\n\nThis note explored the relationship between the t-test, the likelihood ratio test and Fieller's confidence interval. I showed that Fieller's confidence interval for a ratio metric can be derived via the likelihood ratio test, in the same way that Welch's t-test arises for the absolute difference.\n\nThese findings lead me to recommend the following approach when analysing A/B test results:\n\n1. Use Welch's t-test to calculate a p-value, and if the p-value < $\\alpha$, reject the null hypothesis that A and B are equal.\n2. When reporting on the absolute difference, use the standard t-test confidence interval.\n3. When reporting on the relative difference, use Fieller's method to calculate the confidence interval.\n\nThe nice thing about this approach is that all three results will always be aligned. If the p-value does not reject the null hypothesis, then both confidence intervals will include 0, while if the p-value does reject the null hypothesis then neither confidence interval will include 0.\n\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}