[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi there! I‚Äôm Sam Bailey, a Data Scientist at Google, and previously at Booking.com. My day job involves a lot of python, statistics, machine learning and experimentation.\nBut in my free time, I like to explore new data science topics that catch my interest. That usually involves writing some python or julia, whether it‚Äôs coding something from scratch or just testing out a new library.\nAt the end of the day, I try to put my thoughts into words and share them on this blog. Don‚Äôt expect regular posts or peer-reviewed quality, think of this as more of a semi-organised brain-dump. Enjoy!\nThis text was edited by ChatGPT, with the prompt ‚ÄúMake this sound like a light hearted, ‚Äòabout me‚Äô section of a blog‚Äù."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Can we do better than the trimmed mean?\n\n\n33 min\n\n\n\nJulia\n\n\nExperimentation\n\n\nStatistics\n\n\n\nTesting out some ideas for dealing with outliers in A/B testing, comparing the trimmed / winsorized means with a new estimator based on a combination of the Pareto or‚Ä¶\n\n\n\nMay 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to my infrequent, long-winded data science blog!\n\n\n2 min\n\n\n\n\n\n\nMay 1, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Partial blockout experiments for a two sided marketplace\n\n\nBooking.ai (Medium)\n\n\n\n\nData Science\n\n\n\n\nHow can a two sided marketplace run experiments that generate insights for both sides of the marketplace simultaneously? At booking.com I developed an experiment design where we randomise customer + accommodation provider combinations into control and treatment groups, instead of only customers or only accommodation providers. This design generates unique insights, and allowed us to learn the impact a feature is having on both sides of our marketplace from a single experiment.\n\n\n\n\n\n\nJul 1, 2022\n\n\n\n\n\n\n\n\nThe identification of alpha-clustered doorway states in 44,48,52-Ti using machine learning\n\n\nThe European Physical Journal A\n\n\n\n\nNuclear Physics\n\n\n\n\nA novel experimental analysis method has been developed, making use of the continuous wavelet transform and machine learning to rapidly identify alpha-clustering in nuclei in regions of high nuclear state density. This technique was applied to resonant scattering measurements of the 4-He(40,44,48-Ca, alpha) resonant reactions, allowing the ùõº-cluster structure of 44,48,52-Ti to be investigated. Fragmented alpha-clustering was identified in 44-Ti and 52-Ti, while the results for 48-Ti were less conclusive, but suggest no such clustering.\n\n\n\n\n\n\nMar 27, 2021\n\n\n\n\n\n\n\n\nIncreasing the sensitivity of experiments with rank transformation\n\n\nBooking.ai (Medium)\n\n\n\n\nData Science\n\n\n\n\nWhen running online controlled experiments (A/B testing), ensuring that your experiment is sensitive enough to changes in your decision making metric is crucial. However, if your metric is very noisy, this can mean running your A/B tests for a long time, slowing down development velocity. In this project I developed a method to increase the sensitivity of A/B tests, and therefore reduce experiment runtimes, by applying a simple rank transforming on your metric.\n\n\n\n\n\n\nJul 1, 2020\n\n\n\n\n\n\n\n\nExtracting the spectral signature of Œ± clustering in 44,48,52-Ti using a continuous wavelet transform\n\n\nPhysical Review C\n\n\n\n\nNuclear Physics\n\n\n\n\nA novel technique has been developed, making use of the continuous wavelet transform to identify Œ±-clustered states from resonant scattering measurements in regions of high nuclear state density. This technique expedites the investigation of Œ± clustering in medium mass and heavy nuclei where the role that Œ± clustering plays in the structure of the nucleus is poorly understood. Here, we report the application of this technique to measurements of the 4-He(40, 44, 48-Ca, Œ±) resonant reactions, leading to an assessment of the Œ±-cluster structure of 44, 48, 52-Ti. Clustering involving Œ± particles was identified in 44-Ti and 52-Ti but was observed to break down in 48-Ti. The implications of these results are discussed.\n\n\n\n\n\n\nNov 1, 2019\n\n\n\n\n\n\n\n\nAlpha clustering in Ti isotopes: 40,44,48-Ca + Alpha resonant scattering\n\n\nEPJ Web of Conferences\n\n\n\n\nNuclear Physics\n\n\n\n\nMeasurements were made of the 4-He(40,44,48-Ca,Œ±) resonant scattering reactions at 180 degrees and up to E ‚àº11.5 MeV, using the Thick Target Inverse Kinematics technique. These measurements are discussed, with a focus on assessing their usefulness for investigating Œ±-clustering in medium mass 44,48,52-Ti nuclei.\n\n\n\n\n\n\nMar 25, 2016\n\n\n\n\n\n\n\n\nAlpha clustering in 18-F\n\n\nJournal of Physics: Conference Series\n\n\n\n\nNuclear Physics\n\n\n\n\nWe review some of the key experimental and theoretical studies of Œ±-clustering in 18-F. Particular attention is given to the 4p-2h nature of such Œ±-clustered states, and the interaction between the holes and clusters is examined in terms of both weak and strong coupling regimes. The experimental work focuses on Œ±-transfer spectroscopy and Œ± resonant scattering as tools for investigating Œ±-clustering.\n\n\n\n\n\n\nDec 16, 2014\n\n\n\n\n\n\n\n\nEnergy Levels of 18-F from the 14-N + Alpha Resonant Reaction\n\n\nPhysical Review C\n\n\n\n\nNuclear Physics\n\n\n\n\nMeasurements were made of the differential cross section of the resonant reactions 4-He(14-N,Œ±) and, 4-He(14-N,d)16-O with the intention of investigating the compound nucleus 18-F. These measurements were performed in inverse kinematics at a center-of-mass scattering angle of Œ∏ =180 degrees by using a thick 4-He gas target and a 14-N beam. Data were recorded which covered 18-F excitation energies from 5.5‚Äì16 MeV. An R-matrix analysis was performed on the data up to 9 MeV, and the energies, spins, parities, and partial widths were extracted. Nine new states have been identified in 18-F based on this analysis between 8.326 and 8.915 MeV.\n\n\n\n\n\n\nAug 5, 2014\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome to my infrequent, long-winded data science blog!",
    "section": "",
    "text": "Welcome to my data science blog! I‚Äôm pretty surprised you‚Äôre reading this post - I‚Äôm not sure I‚Äôve ever read the ‚Äúwelcome to my blog‚Äù post of any of the many blogs I‚Äôve enjoyed over the years, but as you‚Äôre here let‚Äôs set some expectations, because I‚Äôm doing the complete opposite of what you‚Äôre supposed to do when creating a blog.\nFirstly, I won‚Äôt be posting regularly, so don‚Äôt hold your breath for a new post every week. Secondly, I won‚Äôt be optimizing for SEO because let‚Äôs face it, I don‚Äôt even know what that means. And lastly, my posts won‚Äôt be short and sweet - they might be a bit long and tedious, but hey, that‚Äôs the price of digging deep into a topic.\nSo what will I be doing, you ask? Well, each post will be a Jupyter notebook compiled into a website with Quarto. That‚Äôs right, we‚Äôre not just talking about data science, we‚Äôre doing data science - no fluff pieces or hot takes here, just cold, hard data.\nNow, I must emphasize that nothing is peer-reviewed, so if you try something out from my blog and it all goes wrong, well, that‚Äôs on you my friend. Do your own research and don‚Äôt blame me.\nSo join me on this meandering journey into the world of data science, where we throw caution to the wind and explore the topics that we find interesting, no matter how infrequent or long-winded they may be.\nThis text was written by ChatGPT, after I gave it the talking points."
  },
  {
    "objectID": "posts/trimmed_mean_extreme_value_theory/index.html",
    "href": "posts/trimmed_mean_extreme_value_theory/index.html",
    "title": "Can we do better than the trimmed mean?",
    "section": "",
    "text": "Post insipration\n\n\n\nThis post was heavily inspired by the work by Vehtari et al. (2015) on Pareto Smoothed Importance Sampling, which tackles the related problem of how to deal with extreme values in importance sampling."
  },
  {
    "objectID": "posts/trimmed_mean_extreme_value_theory/index.html#a-day-in-the-life-of-a-data-scientist",
    "href": "posts/trimmed_mean_extreme_value_theory/index.html#a-day-in-the-life-of-a-data-scientist",
    "title": "Can we do better than the trimmed mean?",
    "section": "A day in the life of a data scientist",
    "text": "A day in the life of a data scientist\nPicture the situation: you‚Äôre a data scientist / statistician / economist at an e-commerce company. You‚Äôre working with a team who plan to run an A/B test to evaluate the impact of a new feature or promotion, and critically they want to evaluate the impact of the change on revenue. Sounds easy, so you get to work!\nYour randomisation and analysis units are visitors, and since the goal is to measure the impact on total revenue, your main metric is revenue per visitor. You plan to perform a t-test against the null hypothesis that the average revenue per visitor is the same in both control and treatment, and if you reject the null hypothesis you‚Äôll report the effect size, with a confidence interval, and pop the champaign!\nAs a responsible experimenter, you kick things off with a power analysis, to estimate the required sample size you‚Äôll need for this test. To do this you run a query to get the variance of the revenue per visitor over the last few weeks. And here, things start to go wrong‚Ä¶ your variance is huge! At this rate, you‚Äôll have to run your test for months in order to measure any reasonable effect.\nSo now what? You plot a histogram of your revenue per visitor metric to see what‚Äôs going on, and you see something like this:\n\n\n\n\n\nFigure¬†1: Hypothetical revenue per visitor histogram. Skewed, with lots of zeros on one side and a few massive outliers on the other side.\n\n\n\n\nAnd here‚Äôs the problem. The distribution of the metric is highly skewed, with lots of zeros, then some ‚Äúnormal‚Äù spenders, and then a very small number of extreme spenders - we‚Äôll call these our outliers. And it‚Äôs these outliers that are disproportionately increasing the variance of your average revenue per visitor estimate, hurting your power, and eventually making you wait months to estimate the impact of your experiment."
  },
  {
    "objectID": "posts/trimmed_mean_extreme_value_theory/index.html#outliers-in-ab-testing",
    "href": "posts/trimmed_mean_extreme_value_theory/index.html#outliers-in-ab-testing",
    "title": "Can we do better than the trimmed mean?",
    "section": "Outliers in A/B testing",
    "text": "Outliers in A/B testing\nThe previous section was a hypothetical story, but I think it‚Äôs quite a common scenario. But what can be done? My usual steps are:\n\nFirst, check if the outliers are real. It‚Äôs often the case that outliers are actually just bad data, in which it might be reasonable to exclude them or fix them.\nDo some variance reduction! Cuped is a very effective method of reducing the variance of your metric based on data from before the start of your experiment. It‚Äôs always worth trying and it‚Äôs about as close to a free lunch as you can get when it comes to improving A/B testing power. However, cuped reduces variance, not skew, so it‚Äôs likely your power will still be hindered by the outliers.\nIf the previous two steps doesn‚Äôt reduce your variance sufficiently, then it‚Äôs time to look at trimming or winsorizing. These are very powerful techniques, but come with a large drawback: you are introducing bias into your estimates. This is especially problematic because here we are trimming or winsorizing on one side only (removing extremely high values) - you have no outliers on the lower side of the distribution.\n\n\n\n\n\n\n\nDon‚Äôt log transform!!\n\n\n\n\n\nOn almost every blog I see, when the issue of a skewed metric arises, someone will recommend log-transforming the metric. While this will certainly make your metric less skewed, and may improve power, it‚Äôs a big no for financial metrics like revenue.\nThis is because when you transform your metric, you also change the null hypothesis you are testing and therefore the interpretation of any effect you measure. For revenue, you really care about the effect on the (arithmetic) mean revenue per visitor, because improving this is what will translate into improving total revenue. However, the log transform means you are testing for an effect on the geometric mean, which is not the same and improving it does not necessarily translate into improving your bottom line, leading to potentially bad business decisions.\n\n\n\nTrimming and winsorizing are specifically designed to deal with outliers and make your estimator more robust. With trimming, you filter out all samples that are above some threshold, while with winsorizing you apply a cap, so that all values above a threshold are capped to that threshold. Both are very effective at reducing variance, however because you are removing or capping the top X data points, you introduce a potentially large negative bias, so I usually treat it as a bit of a last resort.\nBut is this the best we can do? In this post I‚Äôm going to explore how we might formalise this problem with extreme value theory, and see if we can find some estimators that perform better than the trimmed or winsorized means. Here, by ‚Äúbetter‚Äù, I mean achieving similar levels of variance reduction, while introducing less bias."
  },
  {
    "objectID": "posts/trimmed_mean_extreme_value_theory/index.html#extreme-value-theory-and-power-laws",
    "href": "posts/trimmed_mean_extreme_value_theory/index.html#extreme-value-theory-and-power-laws",
    "title": "Can we do better than the trimmed mean?",
    "section": "Extreme value theory and power laws",
    "text": "Extreme value theory and power laws\n\n\n\n\n\n\nDisclaimer\n\n\n\nI‚Äôm far from an expert in extreme value theory, I‚Äôve really only learned about the topic in the last few weeks. Take this section with a pinch of salt.\n\n\nExtreme value theory is a branch of statistics dealing with the extreme deviations from the median of probability distributions.1 This is exactly what we are dealing with here - outliers are messing with our statistics. So perhaps there are some tools from extreme value theory that we can use to reduce the bias in our trimmed / winsorized mean? Say hello to the second theorem in extreme value theory:\n\nSecond Theorem in Extreme Value Theory\nFor a wide range of univariate probability distributions, the tail of that distribution can be well modelled by the Generalised Pareto distribution (GPD).\n‚Äì Pickands III (1975), Balkema and De Haan (1974)\n\nThis sounds relevant! Perhaps we can describe the outliers in our skewed revenue distribution as a GPD. So what does a GPD look like?\n\nThe Pareto and Generalised Pareto distributions\nThe GPD is a more flexible version of the well known Pareto distribution (Newman 2005), also known as the 80-20 rule. Both the GPD and Pareto distribution are characterised by having ‚Äúpower law‚Äù tails, where for large X the distribution tends towards:\n\n\n\n\n\n\n\nThe maths\n\n\n\n\n\nThe probability density function of the Pareto distribution is given by:  This distribution has two parameters,  and :\n\n controls the minimum of the distribution: \n controls the slope of the distribution.\n\nThe probability density function of the GPD is given by: \nThe parameters of this distribution are more complex. In this post I‚Äôm going to only look at cases where . In this case:\n\n controls the minimum of the distribution: \n controls the scale of the distribution.\n controls the slope of the power law tail of the distribution. Comparing it to  for the Pareto distribution, .\n\nIn the case where  the GPD is the same as the Pareto distribution.\n\n\n\nYou can see the power law tails in Figure¬†2 below, where I compare the Pareto and GPD distribution with the log-normal distribution, which does not have a power law tail:\n\n\n\n\n\nFigure¬†2: A comparison between the pareto distribution (red), the Generalised Pareto distribution (GPD) (blue), and the log-normal distribution (green). Both axes are log scaled. Pareto has parameters alpha=1.5 and theta=20. GPD has parameters mu=100.0, theta=20.0 and xi=2/3. Log-normal has parameters mu=4.5, sigma=1.0.\n\n\n\n\nThe distributions are plotted on a log-log scale, and you can see that both the Pareto and GPD distributions are linear for large values of  (indicating a power law), while the log-normal distribution never becomes linear, it is always curving downwards.\nThe GPD and Pareto distribution differ at small values of , where the GPD is more flexible and can curve up or down, while the Pareto distribution remains a pure power law for all values of .\n\n\nInfinite Variance\nPower law type distributions are interesting because depending on the slope of the tail, the population might have infinite mean or infinite variance. Specifically for the GPD and Pareto distributions we have the following three cases:\n\n\n\n\n\n\n\n\n\n\n\nPareto\nGPD\n\n\n\n\n1. Infinite Mean and Variance\n\n\n\n\n2. Finite Mean, Infinite Variance\n\n\n\n\n3. Finite Mean and Variance\n\n\n\n\n\nTable¬†1: Three important scenarios for the sample mean and variance of the GPD and Pareto distributions.\nBut what does infinite mean or infinite variance really mean? The sample mean or variance is always finite, even if the population distribution from which that sample was drawn has infinite mean or variance. However, if the population distribution has infinite mean / variance, then the sample mean / variance will not converge as the sample size approaches infinity.\nThis can be seen in Figure¬†3 below, where I simulate estimating the sample variance as a function of sample size for the Log-Normal and GPD distributions, where the GPD distribution has infinite variance. Each grey line represents one sample, where I calculate the sample variance cumulatively, adding one point at a time.\n\n\n\n\n\nFigure¬†3: The sample variance vs sample size for the GPD (left) and log-normal distribution (right). The GPD used here has infinite population variance, so the sample variance does not converge as sample size increases.\n\n\n\n\nYou can see that for the log-normal distribution, as the sample size gets large the variance converges towards the population variance, while for the GPD distribution it does not converge, and instead just keeps increasing.\nThis is a problem, because when performing inference about the mean via the t-test, we construct the t-statistic from the sample mean and sample variance. If, for example, we are performing a one-sample t-test to test the null hypothesis that the mean of the population distribution is equal to a specific value , we would calculate the t-statistic as:\n\nWhere  is the sample mean,  is the sample variance and  is the sample size. Therefore, referring back to the three cases from Table¬†1:\n\nIf we are in case (1) where both the mean and variance are infinite, then there is basically no hope for any inference about the mean, since it‚Äôs infinite! If you have a distribution with such large outliers that you are in this case, then you‚Äôll always struggle to perform any inference about it‚Äôs mean. I‚Äôm not even sure what the interpretation of such an analysis would be‚Ä¶\nIf we are in case (3) then both the mean and variance are finite. This is trivial, and it‚Äôs likely that the regular t-test based on the sample mean and variance will perform fine. Probably your data wouldn‚Äôt even look like it has significant outliers.\nCase (2) is the interesting one. The mean is finite, so we may want to perform inference on it, however the variance is infinite, which will break the t statistic which uses the sample variance.\n\nSo to summarise:\n\nIt‚Äôs common for the tails of a wide variety of distributions to be well described by the GPD distribution.\nUnder certain conditions, the GPD distribution can have infinite variance but a finite mean.\nThis would make any sample drawn from that distribution have a sample variance that does not converge as the sample size increases, and instead the variance will just increase as the sample size increases.\nThis will break out t-statistic, which requires the sample variance.\n\nThis sounds a lot like the situation we have with outliers in our revenue per visitor metric! Perhaps the tail of that distribution follows a power law, where we have infinite variance. This would explain why our variance is so large, and therefore why our experimental power is so poor!"
  },
  {
    "objectID": "posts/trimmed_mean_extreme_value_theory/index.html#inference-about-the-mean-of-a-power-law-distribution",
    "href": "posts/trimmed_mean_extreme_value_theory/index.html#inference-about-the-mean-of-a-power-law-distribution",
    "title": "Can we do better than the trimmed mean?",
    "section": "Inference about the mean of a power law distribution",
    "text": "Inference about the mean of a power law distribution\nSo if we cannot use the sample variance, what can we do? Well really we only use the sample variance so that we can estimated the standard error of the mean, , as:\n\nIs there a way we can do this without the sample variance? Yes, with maximum likelihood estimation and the delta method! This approach takes three steps:\n\nAssume a parametric distribution for your population\nFit your distribution to your data using maximum likelihood estimation\nCalculate the mean and the standard error of the mean from your maximum likelihood fit. This requires calculating the hessian of the log-likelihood of your data, and using the delta method.\n\n\n\n\n\n\n\nDetailed method with maths\n\n\n\n\n\n\nYou have a sample of size : \nAssume a parametric distribution for your population: , where  is a vector of the parameters of the distribution.\nDefine the Log Likelihood as: \nEstimate  by maximising : \nCalculate the hessian matrix of , . This is the matrix of second derivatives of .\nUse the hessian matrix to estimate the covariance matrix for the standard errors of  as , from (Taboga 2021).\nEstimate the maximum likelihood estimate of the mean from , based on the parametric form for the mean of your chosen distribution : .\nUse the delta method to estimate the standard error on the mean, , where  is the first derivative (jacobian) of the mean function:\n\n\nNote: if the parameters are all independent, meaning the covariance matrix is a diagonal matrix, then the above equation simplifies to:\n\nWhere  are the diagonal elements of .\n\n\n\nThis method is nice because it doesn‚Äôt require us to estimate the sample variance at any point, so we are able to estimate the mean and standard error on the mean, even when the population variance is infinite!\nThis method does, however, have some drawbacks - you need to make an assumption about the parametric form of the population distribution, and if that is incorrect it‚Äôs very likely the estimator will be biased. Also, even if the parametric form is correctly specified, it‚Äôs usually the case that the maximum likelihood estimator is not unbiased by default, it usually requires some bias correction to achieve that. This goes beyond the scope of this blog post, but would certainly be possible to explore.\nNow let‚Äôs see how this will work with the GPD and Pareto distributions.\n\nThe Pareto (Hill) estimator\nWhile the more basic Pareto distribution might not have as much flexibility as the GPD, and therefore might not fit the tail of your distribution as well, the maximum likelihood estimate can be calculated analytically, making it much simpler and faster to implement.\nThe maximum likelihood estimator of the Pareto distribution is often known as the Hill estimator, and the parameters are calculated as follows (see Rytgaard (1990) for the derivation):\n \nThe maximum likelihood estimator for the mean and standard error are then calculated from these parameters as:\n\n\n\n\n\n\nDerivation of the mean and standard error\n\n\n\n\n\nThe mean of the Pareto distribution is given by: \nThe parameters are independent, so there is no covariance. Their standard errors are given by Malik (1970) (via wikipedia) as:\n\n\nWe can calculate the standard error on the mean using Equation¬†2:\n\nThe two derivatives are:\n\n\nSo finally substituting it all in we can derive the following equation for the standard error on the mean:\n\n\n\n\n \nThis is implemented in Julia below:\n\nfunction hill_estimator(x::Vector{Float64})\n    n = length(x)\n\n    # Maximum likelihood estimates of the pareto parameters\n    Œ∏ = minimum(x)\n    Œ± = n / (sum(log.(x)) - n * log(Œ∏))\n\n    if Œ± &lt;= 1.0\n        # If alpha is less than 1, the mean is infinite. \n        # In this case this estimator won't work, so fall\n        # back to the sample mean and variance.\n        return sample_mean_variance_estimator(x)\n    end\n\n    # Estimate the mean and standard error from the MLE parameters\n    Œº = Œ± * Œ∏ / (Œ± - 1)\n    œÉ = abs(Œº) * sqrt( 1 / (n * (Œ± - 1)^2) +  1 / (Œ± * n)^2 )\n\n    # Return a struct holding the estimated mean and standard error\n    return EstimatorResult(Œº, œÉ) \nend\n\n\n\nThe GPD estimator\nFor the GPD estimator it‚Äôs more complicated, because there is no closed form analytical solution to the maximum likelihood estimation. This is a well known challenge in extreme value theory and many methods have been proposed to fit the GPD to a sample.\nHere I will be using the method proposed by Zhang and Stephens (2009), which was already implemented in the ParetoSmooth.jl package. I will then use the great automatic differentiation built into Julia in ForwardDiff.jl to estimate the gradient and hessian, which I use to estimate the standard error on the mean via Equation¬†1.\nThe full implementation in Julia is here:\n\nusing ParetoSmooth: gpd_fit\nusing ForwardDiff: gradient, hessian\n\nfunction gpd_log_pdf(params::AbstractVector, x::Vector{Float64})\n    sum(logpdf.(GeneralizedPareto(params...), x))\nend\n\nfunction gpd_mean(params::AbstractVector)\n    mean(GeneralizedPareto(params...))\nend\n\nfunction gpd_estimator(x::Vector{Float64})\n    n = length(x)\n\n    # Estimate the GPD parameters numerically\n    u = minimum(x)\n    x_centered = x .- u\n    Œæ, œÉ = gpd_fit(x_centered, 1.0; sort_sample=true, wip=false)\n    params = [u, œÉ, Œæ]\n\n    if Œæ &gt;= 1.0\n        # If xi is greater than 1, the mean is infinite. \n        # In this case this estimator won't work, so fall\n        # back to the sample mean and variance.\n        return sample_mean_variance_estimator(x)\n    end\n\n    # Perform the automatic differentiation to get the \n    # required gradient and hessian\n    H = hessian(z -&gt; gpd_log_pdf(z, x), params)\n    Œ£ = (-1 * H) ^ -1\n    dŒº = gradient(gpd_mean, params)\n\n    # Estimate the mean and the standard error\n    Œº = gpd_mean(params)\n    v = transpose(dŒº) * Œ£ * dŒº\n\n    # Return a struct holding the estimated mean and standard error\n    return EstimatorResult(Œº, sqrt(v))\nend\n\nAs an aside, this really shows off some of the strengths of Julia - I was able to implement this quite complex method in such a short and easy to read piece of code. However, as it‚Äôs a numerical solution it will run slower and be less stable than the analytical Hill estimator for the Pareto distribution.\n\n\nHow do these estimators perform?\nTo evaluate these estimators, I ran some simulations. First I simulated data coming from the Pareto and GPD distributions, with finite mean but infinite variance. Since I simulate this data I know the true population mean, so I can compare the estimators and see how well they do at estimating the population mean. I then repeat this process many times, each time collecting the estimated population mean and standard error.\nI compare three estimators:\n\nThe sample mean / variance estimator. This is the standard method of estimating the population mean used in the t-test.\nThe Hill estimator. This is the maximum likelihood estimator based on the assumption that the data comes from a Pareto distribution.\nThe GPD estimator. This is the maximum likelihood estimator based on the assumption that the data comes from a GPD distribution.\n\n\nTesting on Pareto data\nFirst let‚Äôs look at how the estimators perform on data that came from a Pareto distribution. As expected, Figure¬†4 shows that the baseline method of using the sample mean doesn‚Äôt work well. It has skewed estimates, with high variance. However, both the hill estimator and the GPD estimator perform much better, with the estimates centred on the true population mean and with much lower variance.\n\n\n\n\n\nFigure¬†4: Distribution of estimates for a Pareto population distribution, with alpha=1.5 and theta=20.\n\n\n\n\nNow we can quantify the total error that the estimator makes as the mean squared error:\n\nThis is a convenient metric to use because we can break it down in terms of bias and variance:\n\n\n\nBelow you can see how the bias, variance and MSE compare. In this case the bias is low for all methods, while the Hill estimator has the lowest variance and MSE, closely followed by the GPD estimator.\n\n\n\n\n\nFigure¬†5: Average performance of estimators for a Pareto population distribution, with alpha=1.5 and theta=20. MSE is the Mean Squared Error of the estimates vs the true population mean, the variance is the variance of the estimates, and the bias is the squared difference between the average of the estimates and the true population mean. Ideally we want bias=0 and then MSE and variance as small as possible.\n\n\n\n\nFinally, just having a good estimate of the mean is not enough - we need to be able to estimate the standard error on the mean reliably for the estimator to be useful for A/B testing.\nTo validate whether the estimate of the standard error is good, I look at the distribution of standardised residuals, which is the difference between the estimated mean and the true population mean, divided by the standard error:\n\nIf the standard errors are good, the standardised residuals should roughly follow a t-distribution with mean = 0, standard deviation = 1 and degrees of freedom = n - 1.\nWe see below that the Hill estimator is good for both sample sizes, while the GPD works well for the larger sample size, but is not as good for the small sample size. The sample mean and variance is poor, again showing the skewed shape.\n\n\n\n\n\nFigure¬†6: A comparison of the standardised residuals of the different estimators for a Pareto population distribution. The t-test assumes that this statistic is approximately t-distributed, with d.o.f = samples size - 1. This expected distribution is shown in red. If the distribution matches, then you can expect the confidence intervals you calculate to have good coverage.\n\n\n\n\nSo overall the Hill and GPD estimators work well for Pareto distributed data, with the Hill estimator performing slightly better. But this is expected, because the data was drawn from the Pareto distribution, which is exactly what the Hill estimator expects. So what happens if the data comes from a GPD distribution?\n\n\nTesting on GPD data\nHere I do the same analysis again, but this time using data coming from a GPD distribution for my testing. This distribution is more complex, so will be a harder test for the Hill estimator, as it breaks the parametric assumption we made.\nFirst looking at the distributions of the mean estimates, we see again that the mean / variance estimator is very skewed, and the GPD estimator looks good. However, this time the Hill estimator is very bad. It‚Äôs not skewed - the estimates have a nice, normally distributed shape, but they are not even nearly centred on the true population mean.\n\n\n\n\n\nFigure¬†7: Distribution of estimates for a GPD population distribution, with mu=100, theta=20.0, xi=2/3.\n\n\n\n\nThis is an example of the bias that can easily be introduced with this method. Because we made an assumption about the parametric form of the population distribution, and got that assumption wrong, the mean estimate is highly biased.\nYou can see it clearly in the MSE and bias results in figure Figure¬†8. The hill estimator has low variance, but high bias, meaning that overall the MSE is much worse than the GPD estimator. Interestingly, even with this high bias it still has a lower MSE than the baseline sample mean / variance estimator, because the variance of the baseline mean/variance estimator is so high! This wouldn‚Äôt hold in general though, probably with a higher sample size or slightly different parameters the hill estimator would perform the worst of all.\n\n\n\n\n\nFigure¬†8: Average performance of estimators for a GPD population distribution, with mu=100, theta=20.0, xi=2/3. MSE is the Mean Squared Error of the estimates vs the true population mean, the variance is the variance of the estimates, and the bias is the squared difference between the average of the estimates and the true population mean. Ideally we want bias=0 and then MSE and variance as small as possible.\n\n\n\n\nFinally we can check the standardised residuals again, and as expected the hill estimator is far away from the expected t-distribution. THe GPD estimator again works best, closely matching the t-distribution for the larger sample size.\n\n\n\n\n\nFigure¬†9: A comparison of the standardised residuals of the different estimators for a GPD population distribution. The t-test assumes that this statistic is approximately t-distributied, with d.o.f = samples size - 1. This expected distribution is shown in red. If the distribution matches, then you can expect the confidence intervals you calculate to have good coverage.\n\n\n\n\nSo it seems that the GPD distribution works better in general, but if the assumptions of the hill estimator are met (i.e.¬†the samples come from a Pareto distribution), then the hill estimator is better. Of course, the GPD estimator will fail in the same way that the hill estimator did if we were to test it on a dataset that breaks its assumptions, it‚Äôs just a more flexible distribution so its assumptions are less likely to break."
  },
  {
    "objectID": "posts/trimmed_mean_extreme_value_theory/index.html#appling-extreme-value-theory-to-the-trimmed-mean",
    "href": "posts/trimmed_mean_extreme_value_theory/index.html#appling-extreme-value-theory-to-the-trimmed-mean",
    "title": "Can we do better than the trimmed mean?",
    "section": "Appling extreme value theory to the trimmed mean",
    "text": "Appling extreme value theory to the trimmed mean\nNow as both the Hill estimator and GDP estimator outperform the simple mean/variance estimator for power-law type data, we can try to apply them to our revenue A/B testing problem. However, we want to use the new estimators only in the tail. The rest of the distribution is unlikely to be power-law distributed, so the new estimators would fail there.\nThis is where the comparison with the trimmed and winsorized means comes in. In both of those methods, an arbitrary threshold is chosen, and all values that are above that threshold are designated as outliers and either ignored or capped. But what if we were to keep the outliers as they are, and use our new estimators on those, while the non-outliers are estimated by the regular mean-variance estimator. We can then combine the two estimates to get the overall estimate of the mean and the standard error for the whole sample.\n\n\n\n\n\n\nDetailed method with maths\n\n\n\n\n\n\nSet the trimming quartile to split the data into ‚Äúoutliers‚Äù and ‚Äúinliers‚Äù. This is an arbitrary percentile of the distribution, and typically only a small percentage of the data should be classified as outliers.\nEstimate the mean and standard error for the inliers and outliers separately. Now you should have the following stats:\n\n\n\n\n\n\nNow we can combine the estimates to get the estimated mean and standard error for the whole sample. This method of combining the results together is built on top of the fact that the overall distribution can be thought of as a mixture of two distributions - the outliers and inliers. So we can combine the moments of the distribution as if they are a mixture (see wikipedia).\n\n\n\nNote - the above equation for  is slightly different to what you‚Äôll see in wikipedia. This is because we are combining the standard error on the mean, not the standard deviations.\n\n\n\nTo test this out, I‚Äôll create a synthetic dataset which is a mixture of a GPD and two lognormal distributions. This will make the main distribution bimodal (and so not too easy for our method) but with a power law tail that gives it infinite variance. This distribution is shown in Figure¬†10.\n\n\n\n\n\nFigure¬†10: Histogram of samples from simulated data (blue), and the theoretical PDF (orange).\n\n\n\n\nAgain I perform very similar simulations to the previous section - estimating the MSE, variance, bias and the MSE of the standardised residuals. In an ideal world, the MSE and variance will be as low as possible, the bias will be 0, and the MSE of the standardised residuals will be as close to 1 as possible. I also calculate the coverage of 95% confidence intervals of the estimates, which is the percentage of times that the true population mean is within the 95% confdience intervals of the estimate. If the estimator is working well it should cover the true value 95% of the time. I calculate all these metrics as a function of the trimming quantile.\nHere I will compare the following estimators:\n\nThe simple mean / variance estimator. This is the standard method of estimating the population mean used in the t-test. It is independent of the trimming quintile.\nThe trimmed mean / variance estimator. The data is trimmed at the trimming quintile before being fed into the sample mean / variance estimator, meaning any samples above the trimming quintile are excluded.\nThe winsorized mean / variance estimator. The data is winsorized at the trimming quintile before being fed into the sample mean / variance estimator, meaning any samples above the trimming quintile are capped at the trimming quintile.\nThe hill tail estimator. This estimator uses the hill estimator for the samples above the trimming quantile, and the sample mean / variance estimator for the samples below, and then merges the results together.\nThe GPD tail estimator. This estimator uses the GPD estimator for the samples above the trimming quantile, and the sample mean / variance estimator for the samples below, and then merges the results together.\n\nThe simulation results are shown below in Figure¬†11.\n\n\n\n\n\nFigure¬†11: Performance of the different trimming methods based on various metrics. The MSE and variance should be as small as possible. The Bias should be as close to 0.0 as possible, the Standardised MSE should be 1.0 and the 95% coverage should be 0.95.\n\n\n\n\nThese results are interesting and in my opinion very promising for this approach!\n\nThe simple mean is the least biased of all the methods, but does not have good coverage of standardised MSE, indicating that the confidence intervals would not work well for this estimator, breaking our A/B tests.\nThe trimmed and winsorized means both behave as expected. The more we trim, the bias increases, and the variance decreases. Overall, the increase in bias is larger than the decrease in variance, meaning that the MSE gets worse. And the bias is so large that the coverage is terrible - the confidence intervals never contain the true population mean. This demonstrates why the trimmed and winsorized means are so difficult to use, because while the variance decreases, you introduce significant bias.\nOur new estimators seem to work well, as long as the trimming quantile is not too large. Both have good coverage and standardised MSE, meaning that can be more reliably used as A/B testing metrics. The variance, bias and MSE are closer to the values for the simple mean. This means we get similar performance on average, but with a better behaved estimator that we can construct reliable confidence intervals for.\nThe hill tail estimator works better than the GPD tail estimator as long as the trimming quintile is kept below ~1%. Above that, the coverage of the hill estimator deteriorates, while the GPD tail estimator maintains its coverage until ~10%. This is expected because the GPD tail estimator is a more general distribution, so can describe a bigger portion of the tail which is not exactly Pareto distributed.\nThe GPD tail estimator is less robust. This is likely because of the numerical estimation method. Between 1% and 10% the variance, bias and MSE go extremely high.\n\nOverall, based on this, I think the hill estimator has the most promise. While it breaks down at high trimming quantiles, as long as the correct trimming quantile is chosen it works well. It is also computationally simpler than the GPD estimator, because it has an analytical solution."
  },
  {
    "objectID": "posts/trimmed_mean_extreme_value_theory/index.html#summary",
    "href": "posts/trimmed_mean_extreme_value_theory/index.html#summary",
    "title": "Can we do better than the trimmed mean?",
    "section": "Summary",
    "text": "Summary\nSo it looks like we might be able to do better than the trimmed or winsorized means! This is still early stages, more work needs to be done to test this, but overall the hill-tail estimator has better coverage than the regular sample mean, while maintaining a good level of bias and variance.\nSome interesting next steps would be to select the trimming quantile automatically, for example using a similar method to Clauset, Shalizi, and Newman (2009), as the hill estimator is very sensitive to this choice, or exploring bias corrections to the MLE estimates."
  },
  {
    "objectID": "posts/trimmed_mean_extreme_value_theory/index.html#execution-details",
    "href": "posts/trimmed_mean_extreme_value_theory/index.html#execution-details",
    "title": "Can we do better than the trimmed mean?",
    "section": "Execution Details",
    "text": "Execution Details\n\n\nInstalled packages:\n\n\nStatsPlots (0.15.5)\nParetoSmooth (0.7.4)\nLaTeXStrings (1.3.0)\nForwardDiff (0.10.35)\nDistributions (0.25.92)\nStatsBase (0.33.21)\n\nExecution time: \n\n\n10 minutes, 18 seconds, 516 milliseconds"
  },
  {
    "objectID": "posts/trimmed_mean_extreme_value_theory/index.html#footnotes",
    "href": "posts/trimmed_mean_extreme_value_theory/index.html#footnotes",
    "title": "Can we do better than the trimmed mean?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSorry, this definition is fully plagiarised from Wikipedia, but it sounds relevant!‚Ü©Ô∏é"
  }
]