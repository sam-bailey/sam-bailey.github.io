[
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Expand To Learn About Collapse\n\n\n\n\n\nThis is an example of a ‘folded’ caution callout that can be expanded by the user. You can use collapse=\"true\" to collapse it by default or collapse=\"false\" to make a collapsible callout that is expanded by default."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Can we do better than the trimmed mean?\n\n\n33 min\n\n\n\nJulia\n\n\nExperimentation\n\n\nStatistics\n\n\n\nTesting out some ideas for dealing with outliers in A/B testing, comparing the trimmed / winsorized means with a new estimator based on a combination of the Pareto or…\n\n\n\nMay 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nHow I’m Breaking All the Rules with My Infrequent, Long-Winded Data Science Blog\n\n\n2 min\n\n\n\n\n\n\nMay 1, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "How I’m Breaking All the Rules with My Infrequent, Long-Winded Data Science Blog",
    "section": "",
    "text": "Welcome to my data science blog, where I explore topics that I find interesting and hope you will too!\nNow, I know what you’re thinking - “Another data science blog? What makes this one so special?” Well, for starters, I’m doing the complete opposite of what you’re supposed to do when creating a blog.\nFirstly, I won’t be posting regularly, so don’t hold your breath for a new post every week. Secondly, I won’t be optimizing for SEO because let’s face it, I don’t even know what that means. And lastly, my posts won’t be short and sweet - they might be a bit long and tedious, but hey, that’s the price of digging deep into a topic.\nSo what will I be doing, you ask? Well, each post will be a Jupyter notebook compiled into a website with Quarto, where I’ll be doing analysis and writing simulations. That’s right - no fluff pieces or hot takes here, just cold, hard data.\nNow, I must emphasize that nothing is peer-reviewed, so if you try something out from my blog and it all goes wrong, well, that’s on you my friend. Do your own research and don’t blame me.\nSo join me on this unconventional journey into the world of data science, where we throw caution to the wind and explore the topics that we find interesting, no matter how infrequent or long-winded they may be.\nThis text was written by ChatGPT, after I gave it the talking points."
  },
  {
    "objectID": "posts/trimmed_mean_extreme_value_theory/index.html",
    "href": "posts/trimmed_mean_extreme_value_theory/index.html",
    "title": "Can we do better than the trimmed mean?",
    "section": "",
    "text": "Post insipration\n\n\n\nThis post was heavily inspired by the work by Vehtari et al. (2015) on Pareto Smoothed Importance Sampling, which tackles the related problem of how to deal with extreme values in importance sampling."
  },
  {
    "objectID": "posts/trimmed_mean_extreme_value_theory/index.html#a-day-in-the-life-of-a-data-scientist",
    "href": "posts/trimmed_mean_extreme_value_theory/index.html#a-day-in-the-life-of-a-data-scientist",
    "title": "Can we do better than the trimmed mean?",
    "section": "A day in the life of a data scientist",
    "text": "A day in the life of a data scientist\nPicture the situation: you’re a data scientist / statistician / economist at an e-commerce company. You’re working with a team who plan to run an A/B test to evaluate the impact of a new feature or promoption, and critically they want to evaluate the impact of the change on revenue. Sounds easy, so you get to work!\nYour randomisation and analysis units are visitors, and since the goal is to measure the impact on total revenue, your main metric is revenue per visitor. You plan to perform a t-test against the null hypothesis that the average revenue per visitor is the same in both control and treatment, and if you reject the null hypothesis you’ll report the effect size, with a confidence interval, and pop the champaign!\nAs a responsible experimenter, you kick things off with a power analysis, to estimate the required sample size you’ll need for this test. To do this you run a query to get the variance of the revenue per visitor over the last few weeks. And here, things start to go wrong… your variance is huge! At this rate, you’ll have to run your test for months in order to measure any reasonable effect.\nSo now what? You plot a histogram of your revenue per visitor metric to see what’s going on, and you see something like this:\n\n\n\n\n\nFigure 1: Hypothetical revenue per visitor histogram. Skewed, with lots of zeros on one side and a few massive outliers on the other side.\n\n\n\n\nAnd here’s the problem. The distribution of the metric is highly skewed, with lots of zeros, then some “normal” spenders, and then a very small number of extreme spenders - we’ll call these our outliers. And it’s these outliers that are disproportionatly increasing the variance of your average revenue per visitor estimate, hurting your power, and eventually making you wait months to estimate the impact of your experiment."
  },
  {
    "objectID": "posts/trimmed_mean_extreme_value_theory/index.html#outliers-in-ab-testing",
    "href": "posts/trimmed_mean_extreme_value_theory/index.html#outliers-in-ab-testing",
    "title": "Can we do better than the trimmed mean?",
    "section": "Outliers in A/B testing",
    "text": "Outliers in A/B testing\nThe previous section was a hypothetical story, but I think it’s quite a common scenario. But what can be done? My usual steps are:\n\nFirst, check if the outlieres are real. It’s often the case that outliers are actually just bad data, in which it might be reasonable to exclude them or fix them.\nDo some variance reduction! Cuped is a very effective method of reducing the variance of your metric based on data from before the start of your experiment. It’s always worth trying and it’s about as close to a free lunch as you can get when it comes to improving A/B testing power. However, cuped reduces variance, not skew, so it’s likely your power will still be hindered by the outliers.\nIf the previous two steps doesn’t reduce your variance sufficiently, then it’s time to look at trimming or windsorizing. These are very powerful techniques, but come with a large drawback: you are introducing bias into your estimates.\n\n\n\n\n\n\n\nDon’t log transform!!\n\n\n\n\n\nOn almost every blog I see, when the issue of a skewed metric arises, someone will recommend log-transforming the metric. While this will certainly make your metric less skewed, and may improve power, it’s a big no for financial metrics like revenue.\nThis is because when you transform your metric, you also change the null hypothesis you are testing and therefore the interpretation of any effect you measure. For revenue, you really care about the effect on the (arithmetic) mean revenue per visitor, because improving this is what will translate into improving total revenue. However, the log transform means you are testing for an effect on the geometric mean, which is not the same and improving it does not necessarily translate into improving your bottom line, leading to potentially bad business decisions.\n\n\n\nTrimming and windsorizing are specifically designed to deal with outliers and make your estimator more robust. With trimming, you filter out all samples that are above some threshold, while with windsorizing you apply a cap, so that all values above a threshold are capped to that threshold. Both are very effective at reducing variance, however because you are removing or capping the top X data points, you introruce a potentially large negative bias, so I usually treat it as a bit of a last resort.\nBut is this the best we can do? In this post I’m going to explore how we might formalize this problem with extreme value theory, and see if we can find some estimators that perform better than the trimmed or winsorized means. Here, by “better”, I mean achieving similar levels of variance reduction, while introducing less bias."
  },
  {
    "objectID": "posts/trimmed_mean_extreme_value_theory/index.html#extreme-value-theory-and-power-laws",
    "href": "posts/trimmed_mean_extreme_value_theory/index.html#extreme-value-theory-and-power-laws",
    "title": "Can we do better than the trimmed mean?",
    "section": "Extreme value theory and power laws",
    "text": "Extreme value theory and power laws\n\n\n\n\n\n\nDisclamer\n\n\n\nI’m far from an expert in extrme value theory, I’ve really only learned about the topic in the last few weeks. Take this section with a pinch of salt.\n\n\nExtreme value theory is a branch of statistics dealing with the extreme deviations from the median of probability distributions.1 This is exactly what we are dealing with here - outliers are messing with our statistics. So perhaps there are some tools from extreme value theory that we can use to reduce the bias in our trimmed / winsorized mean? Say hello to the second theorem in extreme value theory:\n\nSecond Theorem in Extrme Value Theory\nFor a wide range of univariate probability distributions, the tail of that distribution can be well modelled by the Generalised Pareto distribution (GPD).\n– Pickands III (1975), Balkema and De Haan (1974)\n\nThis sounds relavent! Perhaps we can describe the outliers in our skewed revenue distribution as a GPD. So what does a GPD look like?\n\nThe Pareto and Generalised Pareto distributions\nThe GPD is a more flexible version of the well known Pareto distribution (Newman 2005), also known as the 80-20 rule. Both the GPD and Pareto distribution are characterized by having “power law” tails, where for large X the distribution tends towards:\n\n\n\n\n\n\n\nThe maths\n\n\n\n\n\nThe probability density function of the Pareto distribution is given by:  This distribution has two parameters,  and :\n\n controls the minimum of the distribution: \n controls the slope of the distribution.\n\nThe probability density function of the GPD is given by: \nThe parameters of this distribution are more complex. In this post I’m going to only look at cases where . In this case:\n\n controls the minimum of the distribution: \n controls the scale of the distribution.\n controls the slope of the power law tail of the distribution. Comparing it to  for the Pareto distribution, .\n\nIn the case where  the GPD is the same as the Pareto distributon.\n\n\n\nYou can see the power law tails in Figure 2 below, where I compare the Pareto and GPD distribution with the log-normal distribution, which does not have a power law tail:\n\n\n\n\n\nFigure 2: A comparison between the pareto distribution (red), the Generalised Pareto distribution (GPD) (blue), and the log-normal distribution (green). Both axes are log scaled. Pareto has parameters alpha=1.5 and theta=20. GPD has parameters mu=100.0, theta=20.0 and xi=2/3. Log-normal has parameters mu=4.5, sigma=1.0.\n\n\n\n\nThe distributions are plotted on a log-log scale, and you can see that both the Pareto and GPD distributions are linear for large values of  (indicating a power law), while the LogNormal distribution never becomes linear, it is always curving downwards.\nThe GPD and Pareto distribution differ at small values of , where the GPD is more flexible and can curve up or down, while the Pareto distribution remains a pure power law for all values of .\n\n\nInfinite Variance\nPower law type distributions are interesting because depending on the slope of the tail, the population might have infinite mean or infinite variance. Specifically for the GPD and Pareto distributions we have the following three cases:\n\n\n\n\n\n\n\n\n\n\n\nPareto\nGPD\n\n\n\n\n1. Infinite Mean and Variance\n\n\n\n\n2. Finite Mean, Infinite Variance\n\n\n\n\n3. Finite Mean and Variance\n\n\n\n\n\nTable 1: Three important scenarios for the sample mean and variance of the GPD and Pareto distributions.\nBut what does infinte mean or infinite variance really mean? The sample mean or variance is always finite, even if the population distribution from which that sample was drawn has infinite mean or variance. However, if the population distribution has infinite mean / variance, then the sample mean / variance will not converge as the sample size approaches infinty.\nThis can be seen in Figure 3 below, where I simulate estimating the sample variance as a function of sample size for the Log-Normal and GPD distributions, where the GPD distribution has infinite variance. Each grey line represents one sample, where I calculate the sample variance cumulatively, adding one point at a time.\n\n\n\n\n\nFigure 3: The sample variance vs sample size for the GPD (left) and Log-Normal distribution (right). The GPD used here has infinite population variance, so the sample variance does not converge as sample size increases.\n\n\n\n\nYou can see that for the log-normal distribution, as the sample size gets large the variance converges towards the population variance, while for the GPD distribution it does not converge, and instead just keeps increasing.\nThis is a problem, because when performing inference about the mean via the t-test, we construct the t-statistic from the sample mean and sample variance. If, for example, we are performing a one-sample t-test to test the null hypothesis that the mean of the population distribution is equal to a specific value , we would calculate the t-statistic as:\n\nWhere  is the sample mean,  is the sample variance and  is the sample size. Therefore, refering back to the three cases from Table 1:\n\nIf we are in case (1) where both the mean and variance are infinite, then there is basically no hope for any inference about the mean, since it’s infinite! If you have a distribution with such large outliers that you are in this case, then you’ll always struggle to perform any inference about it’s mean. I’m not even sure what the interpretation of such an analysis would be…\nIf we are in case (3) then both the mean and variance are finite. This is trivial, and it’s likely that the regular t-test based on the sample mean and variance will perform fine. Probably you’re data wouldn’t even look like it has significant outliers.\nCase (2) is the interesting one. The mean is finite, so we may want to perform inference on it, however the variance is infinite, which will break the t statisitc which uses the sample variance.\n\nSo to summarise:\n\nIt’s common for the tails of a wide variety of distributions to be well described by the GPD distribution.\nUnder certain conditions, the GPD distribution can have infinite variance but a finite mean.\nThis would make any sample drawn from that distribution have a sample variance that does not converge as the sample size increases, and instead the variance will just increase as the sample size increases.\nThis will break out t-statistic, which requires the sample variance.\n\nThis sounds a lot like the situation we have with outliers in our revenue per visitor metric! Perhaps the tail of that distribution follows a power law, where we have infinite variance. This would explain why our variance is so large, and therefore why our experimental power is so poor!"
  },
  {
    "objectID": "posts/trimmed_mean_extreme_value_theory/index.html#inference-about-the-mean-of-a-power-law-distribution",
    "href": "posts/trimmed_mean_extreme_value_theory/index.html#inference-about-the-mean-of-a-power-law-distribution",
    "title": "Can we do better than the trimmed mean?",
    "section": "Inference about the mean of a power law distribution",
    "text": "Inference about the mean of a power law distribution\nSo if we cannot use the sample variance, what can we do? Well really we only use the sample variance so that we can estimated the standard error of the mean, , as:\n\nIs there a way we can do this without the sample variance? Yes, with maximum likelihood estimation and the delta method! This approach takes three steps:\n\nAssume a parametric distribution for your population\nFit your distribution to your data using maximum likelihood estimation\nCalculate the mean and the standard error of the mean from your maximum likelihood fit. This requires calculating the hessian of the log-likelihood of your data, and using the delta method.\n\n\n\n\n\n\n\nDetailed method with maths\n\n\n\n\n\n\nYou have a sample of size : \nAssume a parametric distribution for your population: , where  is a vector of the parameters of the distribution.\nDefine the Log Likelihood as: \nEstimate  by maximizing : \nCalculate the hessian matrix of , . This is the matrix of second derivatives of .\nUse the hessian matrix to estimate the covariance matrix for the standard errors of  as , from (Taboga 2021).\nEstimate the maximum likelihood estimate of the mean from , based on the parametric form for the mean of your chosen distribution : .\nUse the delta method to estimate the standard error on the mean, , where  is the first derivative (jacobian) of the mean function:\n\n\nNote: if the parameters are all independent, meaning the covariance matrix is a diagonal matrix, then the above equation simplifies to:\n\nWhere  are the diagonal elements of .\n\n\n\nThis method is nice because it doesn’t require us to estimate the sample variance at any point, so we are able to estimate the mean and standard error on the mean, even when the population variance is infinite!\nThis method does, however, have some drawbacks - you need to make an assumption about the parameteric form of the population distribution, and if that is incorrect it’s very likely the estimator will be biased. Also, even if the parametric form is correctly specified, it’s usually the case that the maximum likelihood estimator is not unbiased by default, it usually requires some bias correction to achieve that. This goes beyond the scope of this blog post, but would certainly be possible to explore.\nNow let’s see how this will work with the GPD and Pareto distributions.\n\nThe Pareto (Hill) estimator\nWhile the more basic Pareto distribution might not have as much flexibility as the GPD, and therefore might not fit the tail of your distribution as well, the maximum likelihood estimate can be calculated analytically, making it much simpler and faster to implement.\nThe maximum likelihood estimator of the Pareto distribution is often known as the Hill estimator, and the parameters are calculated as follows (see Rytgaard (1990) for the derivation):\n \nThe maximum likelihood estimator for the mean and standard error are then calculated from these parameters as:\n\n\n\n\n\n\nDerivation of the mean and standard error\n\n\n\n\n\nThe mean of the Pareto distribution is given by: \nThe parameters are independent, so there is no covariance. Their standard errors are given by Malik (1970) (via wikipedia) as:\n\n\nWe can calculate the standard error on the mean using Equation 2:\n\nThe two derivatives are:\n\n\nSo finally substituting it all in we can derive the following equation for the standard error on the mean:\n\n\n\n\n \nThis is implemented in Julia below:\n\nfunction hill_estimator(x::Vector{Float64})\n    n = length(x)\n\n    # Maximum likelihood estimates of the pareto parameters\n    θ = minimum(x)\n    α = n / (sum(log.(x)) - n * log(θ))\n\n    if α &lt;= 1.0\n        # If alpha is less than 1, the mean is infinite. \n        # In this case this estimator won't work, so fall\n        # back to the sample mean and variance.\n        return sample_mean_variance_estimator(x)\n    end\n\n    # Estimate the mean and standard error from the MLE parameters\n    μ = α * θ / (α - 1)\n    σ = abs(μ) * sqrt( 1 / (n * (α - 1)^2) +  1 / (α * n)^2 )\n\n    # Return a struct holding the estimated mean and standard error\n    return EstimatorResult(μ, σ) \nend\n\n\n\nThe GPD estimator\nFor the GPD estimator it’s more complicated, because there is no closed form analytical solution to the maximum likelihood estimation. This is a well known challenge in extreme value theory and many methods have been proposed to fit the GPD to a sample.\nHere I will be using the method proposed by Zhang and Stephens (2009), which was already implemented in the ParetoSmooth.jl package. I will then use the great automatic differentiation built into Julia in ForwardDiff.jl to estimate the gradient and hessian, which I use to estimate the standard error on the mean via Equation 1.\nThe full implementation in Julia is here:\n\nusing ParetoSmooth: gpd_fit\nusing ForwardDiff: gradient, hessian\n\nfunction gpd_log_pdf(params::AbstractVector, x::Vector{Float64})\n    sum(logpdf.(GeneralizedPareto(params...), x))\nend\n\nfunction gpd_mean(params::AbstractVector)\n    mean(GeneralizedPareto(params...))\nend\n\nfunction gpd_estimator(x::Vector{Float64})\n    n = length(x)\n\n    # Estimate the GPD parameters numerically\n    u = minimum(x)\n    x_centered = x .- u\n    ξ, σ = gpd_fit(x_centered, 1.0; sort_sample=true, wip=false)\n    params = [u, σ, ξ]\n\n    if ξ &gt;= 1.0\n        # If xi is greater than 1, the mean is infinite. \n        # In this case this estimator won't work, so fall\n        # back to the sample mean and variance.\n        return sample_mean_variance_estimator(x)\n    end\n\n    # Perform the automatic differentiation to get the \n    # required gradient and hessian\n    H = hessian(z -&gt; gpd_log_pdf(z, x), params)\n    Σ = (-1 * H) ^ -1\n    dμ = gradient(gpd_mean, params)\n\n    # Estimate the mean and the standard error\n    μ = gpd_mean(params)\n    v = transpose(dμ) * Σ * dμ\n\n    # Return a struct holding the estimated mean and standard error\n    return EstimatorResult(μ, sqrt(v))\nend\n\nAs an aside, this really shows off some of the strengths of Julia - I was able to implement this quite complex method in such a short and easy to read piece of code. However, as it’s a numerical solution it will run slower and be less stable than the analytical Hill estimator for the Pareto distribution.\n\n\nHow do these estimators perform?\nTo evaluate these estimators, I ran some simulations. First I simulated data coming from the Pareto and GPD distributions, with finite mean but infinite variance. Since I simulate this data I know the true population mean, so I can compare the estimators and see how well they do at estimating the population mean. I then repeat this process many times, each time collecting the estimated population mean and standard error.\nI compare three estimators:\n\nThe sample mean / variance estimator. This is the standard method of estimating the population mean used in the t-test.\nThe Hill estimator. This is the maximum likelihood estimator based on the assumption that the data comes from a Pareto distribution.\nThe GPD estimator. This is the maximum likelihood estimator based on the assumption that the data comes from a GPD distribution.\n\n\nTesting on Pareto data\nFirst let’s look at how the estimators perform on data that came from a Pareto distribution. As expected, Figure 4 shows that the baseline method of using the sample mean doesn’t work well. It has skewed estimates, with high variance. However, both the hill estimator and the GPD estimator perform much better, with the estimates centered on the true population mean and with much lower variance.\n\n\n\n\n\nFigure 4: Distribution of estimates for a Pareto population distribution, with alpha=1.5 and theta=20.\n\n\n\n\nNow we can quantify the total error that the estimator makes as the mean squared error:\n\nThis is a convenient metric to use because we can break it down in terms of bias and variance:\n\n\n\nBelow you can see how the bias, variance and MSE compare. In this case the bias is low for all methods, while the Hill estimator has the lowest variance and MSE, closely followed by the GPD estimator.\n\n\n\n\n\nFigure 5: Average performance of estimators for a Pareto population distribution, with alpha=1.5 and theta=20. MSE is the Mean Squared Error of the estimates vs the true population mean, the variance is the variance of the estimates, and the bias is the squared difference between the average of the estimates and the true population mean. Ideally we want bias=0 and then MSE and variance as small as possible.\n\n\n\n\nFinally, just having a good estimate of the mean is not enough - we need to be able to estimate the standard error on the mean reliably for the estimator to be useful for A/B testing.\nTo validate whether the estimate of the standard error is good, I look at the distribution of standardised residuals, which is the difference between the estimated mean and the true population mean, divided by the standard error:\n\nIf the standard errors are good, the standardised residuals should roughly follow a t-distribution with mean = 0, standard deviation = 1 and degrees of freedom = n - 1.\nWe see below that the Hill estimator is good for both sample sizes, while the GPD works well for the larger sample size, but is not as good for the small sample size. The sample mean and variance is poor, again showing the skewed shape.\n\n\n\n\n\nFigure 6: A comparison of the standardised errors of the different estimators for a Pareto population distribution. The t-test assumes that this statistic is approximately t-distributied, with d.o.f = samples size - 1. This expected distribution is shown in red. If the distribution matches, then you can expect the confidence intervals you calculate to have good coverage.\n\n\n\n\nSo overall the Hill and GPD estimators work well for Pareto distributed data, with the Hill estimator performing slightly better. But this is expected, because the data was drawn from the Pareto distribution, which is exactly what the Hill estimator expects. So what happens if the data comes from a GPD distribution?\n\n\nTesting on GPD data\nHere I do the same analysis again, but this time using data coming from a GPD distribution for my testing. This distribution is more complex, so will be a harder test for the Hill estimator, as it breaks the parametric assumption we made.\nFirst looking at the distributions of the mean estimates, we see again that the mean / variance estimator is very skewed, and the GPD estimator looks good. However, this time the Hill estimator is very bad. It’s not skewed - the estimates have a nice, normally distributed shape, but they are not even nearly centered on the true population mean.\n\n\n\n\n\nFigure 7: Distribution of estimates for a GPD population distribution, with mu=100, theta=20.0, xi=2/3.\n\n\n\n\nThis is an example of the bias that can easily be introduced with this method. Becuase we made an assumption about the parametric form of the population distribution, and got that assumption wrong, the mean estimate is highly biased.\nYou can see it clearly in the MSE and bias results in figure Figure 8. The hill estimator has low variance, but high bias, meaning that overall the MSE is much worse than the GPD estimator. Interestingly, even with this high bias it still has a lower MSE than the baseline sample mean / variance estimator, because the variance of the baseline mean/variance estimator is so high! This wouldn’t hold in general though, probably with a higher sample size or slightly different parameters the hill estimator would perform the worst of all.\n\n\n\n\n\nFigure 8: Average performance of estimators for a GPD population distribution, with mu=100, theta=20.0, xi=2/3. MSE is the Mean Squared Error of the estimates vs the true population mean, the variance is the variance of the estimates, and the bias is the squared difference between the average of the estimates and the true population mean. Ideally we want bias=0 and then MSE and variance as small as possible.\n\n\n\n\nFinally we can check the standardised residuals again, and as expected the hill estimator is far away from the expected t-distribution. THe GPD estimator again works best, closely matching the t-distribution for the larger sample size.\n\n\n\n\n\nFigure 9: A comparison of the standardised errors of the different estimators for a GPD population distribution, where the standardised error is the difference between the estimated mean and the population mean, divided by the estimated standard deviation. The t-test assumes that this statistic is approximately t-distributied, with d.o.f = samples size - 1. This expected distribution is shown in red. If the distribution matches, then you can expect the confidence intervals you calculate to have good coverage.\n\n\n\n\nSo it seems that the GPD distribution works better in general, but if the assumptions of the hill estimator are met (i.e. the samples come from a Pareto distribution), then the hill estimator is better. Of course, the GPD estimator will fail in the same way that the hill estimator did if we were to test it on a dataset that breaks it’s assumptions, it’s just a more flexible distribution so its assumptions are less likely to break."
  },
  {
    "objectID": "posts/trimmed_mean_extreme_value_theory/index.html#appling-extreme-value-theory-to-the-trimmed-mean",
    "href": "posts/trimmed_mean_extreme_value_theory/index.html#appling-extreme-value-theory-to-the-trimmed-mean",
    "title": "Can we do better than the trimmed mean?",
    "section": "Appling extreme value theory to the trimmed mean",
    "text": "Appling extreme value theory to the trimmed mean\nNow as both the Hill estimator and GDP estimator outperform the simple mean/variance estimator for power-law type data, we can try to apply them to our revenue A/B testing problem. However, we want to use the new estimators only in the tail. The rest of the distribution is unlikely to be power-law distributed, so the new estimators would fail there.\nThis is where the comparison with the trimmed and windsorised means comes in. In both of those methods, an arbitrary threshold is chosen, and all values that are above that threshold are designated as outliers and either ignored or capped. But what if we were to keep the outliers as they are, and use our new estimators on those, while the non-outliers are estimated by the regular mean-variance estimator. We can then combine the two estimates to get the overall estimate of the mean and the standard error for the whole sample.\n\n\n\n\n\n\nDetailed method with maths\n\n\n\n\n\n\nSet the trimming quartile to split the data into “outliers” and “inliers”. This is an arbitrary percentile of the distribution, and typically only a small percentage of the data should be classified as outliers.\nEstimate the mean and standard error for the inliers and outliers separately. Now you should have the following stats:\n\n\n\n\n\n\nNow we can combine the estimates to get the estimated mean and standard error for the whole sample. This method of combining the results together is built on top of the fact that the overall distribution can be thought of as a mixture of two distributions - the outliers and inliers. So we can combine the moments of the distribution as if they are a mixture (see wikipedia).\n\n\n\nNote - the above equation for  is slightly different to what you’ll see in wikipedia. This is because we are combining the standard error on the mean, not the standard deviations.\n\n\n\nTo test this out, I’ll create a synthetic dataset which is a mixture of a GPD and two lognormal distributions. This will make the main distribution bimodal (and so not too easy for our method) but with a power law tail that gives it infinite variance. This distribution is shown in Figure 10.\n\n\n\n\n\nFigure 10: Histogram of samples from simulated data (blue), and the theoretical PDF (orange).\n\n\n\n\nAgain I perform very similar simulations to the previous section - estimating the MSE, variance, bias and the MSE of the standardised residuals. In an ideal world, the MSE and variance will be as low as possible, the bias will be 0, and the MSE of the standardised residuals will be as close to 1 as possible. I also calculate the coverage of 95% confidence intervals of the estimates, which is the percentage of times that the true population mean is within the 95% confdience intervals of the estimate. If the estimator is working well it should cover the true value 95% of the time. I calculate all these metrics as a function of the trimming quantile.\nHere I will compare the following estimators:\n\nThe simple mean / variance estimator. This is the standard method of estimating the population mean used in the t-test. It is independent of the trimming quintile.\nThe trimmed mean / variance estimator. The data is trimmed at the trimming quintile before being fed into the sample mean / variance estimator, meaning any samples above the trimming quintile are excluded.\nThe winsorized mean / variance estimator. The data is winsorized at the trimming quintile before being fed into the sample mean / variance estimator, meaning any samples above the trimming quintile are capped at the trimming quintile.\nThe hill tail estimator. This estimator uses the hill estimator for the samples above the trimming quntile, and the sample mean / variance estimator for the samples below, and then merges the results together.\nThe GPD tail estimator. This estimator uses the GPD estimator for the samples above the trimming quntile, and the sample mean / variance estimator for the samples below, and then merges the results together.\n\nThe simulation results are shown below in Figure 11.\n\n\n\n\n\nFigure 11: Performance of the different trimming methods based on various metrics. The MSE and variance should be as small as possible. The Bias should be as close to 0.0 as possible, the Standardised MSE should be 1.0 and the 95% coverage should be 0.95.\n\n\n\n\nThese results are interesting and in my opinion very promising for this approach!\n\nThe simple mean is the least biased of all the methods, but does not have good coverage of standardised MSE, indicating that the confidence intervals would not work well for this estimator, breaking our A/B tests.\nThe trimmed and winsorized means both behave as expected. The more we trim, the bias increases, and the variance decreases. Overall, the increase in bias is larger than the decrease in variance, meaning that the MSE gets worse. And the bias is so large that the coverage is terrible - the confidence intervals never contain the true population mean. This demonstrates why the trimmed and winsorized means are so difficult to use, because while the variance decreases, you introduce significant bias.\nOur new estimators seem to work well, as long as the trimming quntile is not too large. Both have good coverage and standardised MSE, meaning that can be more reliably used as A/B testing metrics. The variance, bias and MSE are closer to the values for the simple mean. This means we get similar performance on average, but with a better behaved estimator that we can construct reliable confidence intervals for.\nThe hill tail estimator works better than the GPD tail estimator as long as the trimming quintile is kept below ~1%. Above that, the coverage of the hill estimator deteriorates, while the GPD tail estimator maintains its coverage until ~10%. This is expected because the GPD tail estimator is a more general distribution, so can describe a bigger portion of the tail which is not exactly Pareto distributed.\nThe GPD tail estimator is less robust. This is likely because of the numerical estimation method. Between 1% and 10% the variance, bias and MSE go extremely high.\n\nOverall, based on this, I think the hill estimator has the most promise. While it breaks down at high trimming quantiles, as long as the correct trimming quantile is chosen it works well. It is also computationally simpler than the GPD estimator, because it has an analytical soltuion."
  },
  {
    "objectID": "posts/trimmed_mean_extreme_value_theory/index.html#summary",
    "href": "posts/trimmed_mean_extreme_value_theory/index.html#summary",
    "title": "Can we do better than the trimmed mean?",
    "section": "Summary",
    "text": "Summary\nSo it looks like we might be able to do better than the trimmed or winsorized means! This is still early stages, more work needs to be done to test this, but overall the hill-tail estimator has better coverage than the regular sample mean, while maintaining a good level of bias and variance.\nSome interesting next steps would be to select the trimming quantile automatically, for example using a similar method to Clauset, Shalizi, and Newman (2009), as the hill estimator is very sensitive to this choice, or exploring bias corrections to the MLE estimates."
  },
  {
    "objectID": "posts/trimmed_mean_extreme_value_theory/index.html#execution-details",
    "href": "posts/trimmed_mean_extreme_value_theory/index.html#execution-details",
    "title": "Can we do better than the trimmed mean?",
    "section": "Execution Details",
    "text": "Execution Details\n\n\nInstalled packages:\nStatsPlots (\n\n\n0.15.5)\nParetoSmooth (0.7.4)\nLaTeXStrings (1.3.0)\nForwardDiff (0.10.35)\nDistributions (0.25.92)\nStatsBase (0.33.21)\n\nExecution time: \n\n\n10 minutes, 15 seconds, 685 milliseconds"
  },
  {
    "objectID": "posts/trimmed_mean_extreme_value_theory/index.html#footnotes",
    "href": "posts/trimmed_mean_extreme_value_theory/index.html#footnotes",
    "title": "Can we do better than the trimmed mean?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSorry, this definition is fully plagarised from Wikipedia, but it sounds relevant!↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi there! I’m Sam Bailey, and I’m a Principal Data Scientist at Booking.com. My day job involves a lot of python, statistics, and randomised controlled trials.\nBut in my free time, I like to explore new topics that catch my interest. That usually involves writing some python or julia, whether it’s coding something from scratch or just testing out a new library.\nAt the end of the day, I try to put my thoughts into words and share them on this blog. I hope you find my musings interesting and maybe even useful! Thanks for stopping by.\nThis text was edited by ChatGPT, with the prompt “Make this sound like a light hearted, ‘about me’ section of a blog”."
  }
]