---
title: "Trimming Outliers for A/B Testing"
date: "2022-12-08"
categories: [Julia]
image: ../../_freeze/posts/outliers-for-ab-testing/index/figure-html/fig-samples-output-1.svg
jupyter: "julia-1.8"
bibliography: references.bib
cap-location: margin
---

```{julia}
#| echo: false
#| output: false

using Pkg
Pkg.add([
	"Random",
	"Distributions",
	"Optim",
	"StatsBase",
	"HypothesisTests",
	"StatsPlots",
	"LaTeXStrings"
])

using Random, Distributions, Optim, StatsBase, HypothesisTests
using StatsPlots, LaTeXStrings

Random.seed!(123);
```

* Running A/B tests targeting continuous metrics like revenue per visitor are important for companies to evaluate the financial impact of their interventions
* These metrics are often hard from a statistical perspective because they are influenced by outliers, which dramatically hurt experimental power
* One way to think about these outliers is to say you don't want to optimize your website for a minority of extremely high value customers, since this is high risk. This thinking goes towards the ideas of quantitative finance, and I won't explore this in this post.
* Another reason could be data issues. Lets assume we rule that out.
* The second way is to say you want to optimize your website for everyone, so you can't ignore the outliers, but their existance makes standard statistical tests inefficient, so we try to find more efficient estimators that require introducing as little bias as possible.
* We can formalise this by assuming that the majority of our sample comes from some well behaved population distribution, but a small number of samples are drawn from a population distribution with infinite variance. For example, the Pareto distribution [@powerlaws].
* This contribution from the infinite variance distribution is a problem, because it causes the mixture distribution to also have infinite variance, and therefore the central limit theorem breaks down. 

TODO:
* Compare trimming, winzoring and pareto smoothing with different cutoffs
* Use KDE + Pareto mixture. Then take the mean of the MLE pareto + mean of other data for estimator. Use laplace approx for variance.
* Basic check: is Hill better at estimating mean than sample mean?

SECTIONS:
1. Introduction on A/B testing, revenue as a metric, and outliers. Talk about outliers as coming from a distribution with infinite population variance.
2. Infinite population variance: the Pareto distribution. What does infinite variance mean? Why is it a problem for A/B testing? Do a simulation with a simple welches t-test.
3. What can we not do? Log transform, median test, non-parametric test. We care about the mean. 
4. Simple solutions: trimming vs windsorizing. These are a bias variance tradeoff. How do they perform? Do the same simulations.
5. What if we model the tail as a pareto distribution directly? Try PSIS and a KDE + Pareto Mixture for MLE. 
6. Choosing the best cutoff? KS test, AIC, BIC, Mixture? Mixture doesn't need a cutoff - that's nice. 
7. Final comparison and results. My recommendation. Key points - choose the cutoff without looking at the results to avoid peaking.  

Method:

The following is inspired by the Pareto Smoothed Importance Sampling paper [@psis]:

1. Set $M = min(0.2 S, 3 \sqrt{S})$ OR find best M using KS-test [@powerlawsempirical];
2. Set $\hat{u} = x_{S-M}$
3. Estimate $\alpha$ of the tail, using the standard Hill estimator
4. Set $x'_{S-M+z} = \min{ \left( F^{-1} \left( \frac{z - 1/2}{M} \right), \max_s{(x_s)} \right)}$

Now perform a t-test on $x'$ instead of $x$. 

Hill estimator:

$$
\alpha = 1 + n \left[\sum_{i=1}^n \ln{ \frac{x_i}{x_\text{min}} } \right]^{-1}
$$

Throughout this article we will use two types of distributions to represent distributions with infinite variance, the Pareto distribution and the Generalised Pareto Distribution (GPD). 

```{julia}
#| label: fig-pareto
#| fig-cap: "A comparison between the standard pareto distribution (red), the Generalised Pareto Distribution (blue), and the log-normal distribution (green). "
gpd = GeneralizedPareto(0.0, 20.0, 0.7)
pd = Pareto(1.0 / 0.7, 20.0)
log_norm = LogNormal(4.5, 1.0)

plot(gpd, label="Generalised Pareto")
plot!(pd, label="Pareto")
plot!(log_norm, label="Log-Normal")
plot!(xscale=:log10, yscale=:log10, ylabel="P(x)", xlabel="x", xrange=[1.0; 10000.0], yrange=[0.0000001; 0.4])
```

Both of these distributions have a power law tail, and therefore can have infinite variance. This is noticable when plotted on a log-log scale (as is shown in @fig-pareto), where the tail will be linear. This is shown in contrast to a Log-Normal distribution, where the tail is not Linear, and eventually at very high $X$ the PDF drops below the power law tails. 

While the two types of pareto distribution have the same tail behavior, they differ in how they behave at small values of $X$. The Pareto distribution is a pure power law until some value, below which the probability mass is 0, while the Generalised Pareto Distribution smoothly transitions away from being power law at low X. The GPD is a more general distribution - with the correct parameters in the GPD you can get back to the Pareto distribution - however it can be harder to fit to data because it has more parameters. 

The challenge for A/B testing can be demonstrated by comparing these distributions with the log-normal distribution. While you are always able to calculate the sample variance of any sample of data, whether it is from a population with finite or infinite variance, if the sample is drawn from a population with infinite variance then the sample variance will not converge as the sample size increases. You can see this in the example below, where I sample from a GPD and a LogNormal Distribution and then calculate the sample variance vs sample size. 

```{julia}
#| label: fig-inf-variance
#| fig-cap: ""

function cumvar(x::Vector{Float64})
	sum_x = cumsum(x)
	sum_x2 = cumsum(x.^2)
	n = 1:length(x)

	var = sum_x2 ./ n - (sum_x ./ n).^2
	return var
end

function plot_var_vs_samplesize(dist::UnivariateDistribution, max_samples::Int64, title::String)
	p = plot(
		xscale = :log10, 
		yscale=:log10, 
		xlabel="Sample Size", 
		ylabel="Sample Variance", 
		yrange=[0.5, 1e8], 
		title=title
	)
	samples = [cumvar(rand(dist, max_samples)) for i in 1:20]

	plot!(
		1:max_samples, 
		samples[1], 
		color=:black, 
		alpha=0.2, 
		label="Samples"
	)
	plot!(
		1:max_sample_size, 
		samples[2:length(samples)], 
		color=:black, 
		alpha=0.2, 
		label=""
	)
	return p
end

max_sample_size = 100000

p1 = plot_var_vs_samplesize(gpd, max_sample_size, "GPD")
p2 = plot_var_vs_samplesize(log_norm, max_sample_size, "Log-Normal")
hline!([var(log_norm)], color=:red, label="Population Variance")

plot(p1, p2)
```

This causes problems for the t-test, since it uses the sample variance to estimate the standard error on the mean. So if your data has a power law tail, it's unlikely that this will give a good estimate. 

Power law tails are usualy quantified using the Hill Estimator. We can get the standard error on that estimate too. Let's compare the Hill estimator + Delta method with the mean and variance estimator for the regular Pareto distribution.

```{julia}
#| label: fig-hill-estimator
#| fig-cap: "Estimating the mean and the standard error for samples taken from a Pareto Distribution. Using the sample mean and variance (left) and using the Hill estimator (right), for two sample sizes: 200 (top) and 10000 (bottom). Compared with the expected t-distribtuion with d.o.f = samples size - 1."
function hill_estimator(x::Vector{Float64})
	n = length(x)

	u = minimum(x)
	α = n / (sum(log.(x)) - n * log(u))

	μ = α * u / (α - 1)
	σ = μ * sqrt(1.0 / ((α - 1)^2 * n) + (α - 1)^2 / (α^2 * n))  # CHECK THIS DERIVATION PROPERLY

	return μ, σ
end

function sample_mean_variance_estimator(x::Vector{Float64})
	μ = mean(x)
	n = length(x)
	σ = std(x) / sqrt(n)

	return μ, σ
end

function simulate_pareto_estimators(
	pop_dist::UnivariateDistribution, 
	n_samples::Int64,
	n_iterations::Int64,
	estimator
	)
	μ_true = mean(pop_dist)
	return [
		begin
			x = rand(pop_dist, n_samples)
			μ, σ = estimator(x)
			(μ - μ_true) / σ
		end
		for i in 1:n_iterations
	]
end

function plot_pareto_estimators(
	pop_dist::UnivariateDistribution, 
	n_samples::Int64,
	n_iterations::Int64,
	estimator,
	title::String;
	legend::Bool = false
)
	results = simulate_pareto_estimators(
		pop_dist,
		n_samples,
		n_iterations,
		estimator
	)
	print(title)
	print(" ")
	print(mean(results))
	print(" ")
	println(std(results))

	p = histogram(
		results, 
		normalize=true, 
		title=title, 
		label=L"(\hat{\mu} - \mu) / \hat{\sigma}",
		legend=legend,
		titlefontsize=10
	)
	plot!(TDist(n_samples-1), label="Expected Distribution")

	return p
end

l = @layout [[a;b] [c;d]]

p1 = plot_pareto_estimators(
	pd,
	200,
	5000,
	sample_mean_variance_estimator,
	"Sample Mean / Var Estimator\n(n=200)",
	legend=true
)

p2 = plot_pareto_estimators(
	pd,
	200,
	5000,
	hill_estimator,
	"Hill Estimator\n(n=200)"
)

p3 = plot_pareto_estimators(
	pd,
	10000,
	5000,
	sample_mean_variance_estimator,
	"Sample Mean / Var Estimator\n(n=10,000)"
)

p4 = plot_pareto_estimators(
	pd,
	10000,
	5000,
	hill_estimator,
	"Hill Estimator\n(n=10,000)"
)

plot(p1, p3, p2, p4, layout=l)

```

You can see that the hill estimator performs much better than using the same mean and variance. 

So we want to use the Hill estimator instead of the sample mean and variance, but **only in the tail**. The rest of the distribution is unlikely to be pareto distributed, so the Hill estimator would fail there. This brings us to the next challenge: how do we decide when we are "in the tail"?

To do this, we will use a semi-parametric mixture of the Pareto distribution in the tail and a Kernel Density Approximation (KDE) elsewhere. The KDE bandwidth and cutoff threshold for the tail will be chosen by maximizing the Leave-One-Out (LOO) likelihood.

To test this out, I'll create a synthetic dataset which is a mixture of a GPD and two lognormal distributions. This will make the main distribution bimodal (and so not too easy for our method) but with a power law tail that gives it infinite variance. This distribution is shown in @fig-samples. 

```{julia}
#| label: fig-samples
#| fig-cap: "Histogram of samples from simulated data (blue), and the theoretical PDF (orange)."

data_dist = MixtureModel(
	[
	   LogNormal(2.0, 0.5),
	   LogNormal(3.8, 0.5),
	   GeneralizedPareto(0.0, 40.0, 0.7)
	], 
	[0.5, 0.4, 0.1]
);

x_samples = rand(data_dist, 1000);

x = LinRange(0.001,200,1000)
y_pdf = pdf.(data_dist, x)

p1 = histogram(
     x_samples, 
     normalize=true, 
     label="Samples"
)
plot!(x,y_pdf,label="PDF", lw=2)
xlabel!("X")

x = 10.0.^LinRange(-1,3,1000)
y_pdf = pdf.(data_dist, x)
y_comp = [pdf.(component(data_dist,i), x) .* probs(data_dist)[i] for i in 1:ncomponents(data_dist)]

p2 = plot(x,y_pdf,label="Mixture PDF", lw=2)
plot!(x, y_comp, lw=1, label=["Log-Norm 1" "Log-Norm 2" "GPD"])
plot!(yscale=:log10, xscale=:log10, yrange=[1e-6, 1.0])
xlabel!("X")

plot(p1,p2)
```

Now I can build my method to estimate the mean of that distribution, and the standard error on the mean. 

Now lets investigate the tail of this distribution. Does it look like a power law? Can we fit one to it?

1. Set the tail threshold as the smallest possible value where the KS test p_value >= 0.05
1. Calculate mean and standard error using Hill estimator for all samples in the tail
2. Calculate normal mean and standard error for all samples, also based on their weights
3. Combine the two estimates to get an overal mean and standard error

ITS CLOSE BUT I MUST HAVE SLIGHTLY THE WRONG FORMULAS

```{julia}
# function fit_pareto(log_x::Vector{Float64})
# 	n = length(log_x)
# 	log_θ = minimum(log_x)
# 	α = n / (sum(log_x) - n * log_θ)

# 	return Pareto(α, exp(log_θ))
# end

function set_tail_index(x::Vector{Float64})
	n = length(x)
	trunc(Int, minimum([0.2 * n, 3.0 * sqrt(n)]))
end

function hill_tail_estimator(x_in::Vector{Float64})
	x = sort(x_in, rev=true)
	tail_index = set_tail_index(x)

	x_tail = x[1:tail_index]
	x_non_tail = x[tail_index+1:length(x)]

	μ_tail, σ_tail = hill_estimator(x_tail)
	μ_non_tail, σ_non_tail = sample_mean_variance_estimator(x_non_tail)

	n_tail = tail_index
	n = length(x)
	n_non_tail = n - μ_non_tail

	μ = μ_tail * n_tail / n + μ_non_tail * n_non_tail / n

	var_tail = σ_tail^2 * n_tail
	var_non_tail = σ_non_tail^2 * n_non_tail
	var = (
		n_tail / n * (var_tail + μ_tail^2)
		+ n_non_tail / n * (var_non_tail + μ_non_tail^2)
		- μ^2
	)

	σ = sqrt(var / n)

	return μ, σ
end

function windzorised_estimator(x_in::Vector{Float64})
	x = sort(x_in, rev=true)
	tail_index = set_tail_index(x)

	x[1:tail_index] .= x[tail_index]
	return sample_mean_variance_estimator(x)
end

function trimmed_estimator(x_in::Vector{Float64})
	x = sort(x_in, rev=true)
	tail_index = set_tail_index(x)

	return sample_mean_variance_estimator(x[tail_index+1:length(x)])
end

l = @layout [[a;b] [c;d]]

p1 = plot_pareto_estimators(
	data_dist,
	20000,
	1000,
	sample_mean_variance_estimator,
	"Sample Mean / Var Estimator",
	legend=true
)

p2 = plot_pareto_estimators(
	data_dist,
	20000,
	1000,
	trimmed_estimator,
	"Trimmed Estimator"
)

p3 = plot_pareto_estimators(
	data_dist,
	20000,
	1000,
	windzorised_estimator,
	"Windsorized Estimator"
)

p4 = plot_pareto_estimators(
	data_dist,
	20000,
	1000,
	hill_tail_estimator,
	"Hill Tail Estimator"
)

plot(p1, p2, p3, p4, layout=l)
```


```{julia}
# function calculate_cumulative_ks(x_in::Vector{Float64})
# 	x = sort(x_in, rev=true)
# 	w = ones(Float64, size(x))
# 	log_x = log.(x)

# 	ks = [
# 		begin
# 			pareto = fit_pareto(log_x[1:m], w[1:m])
# 			pvalue(ExactOneSampleKSTest(x[1:m], pareto))
# 		end
# 		for m in 1:length(x)
# 	]

# 	return ks
# end

# ks = calculate_cumulative_ks(x_samples)

# p1 = plot(sort(x_samples, rev=true), qks, label="p-value")

# max_idx = findfirst(ks .< 0.05)
# println(max_idx)
# threshold = sort(x_samples, rev=true)[max_idx]
# vline!([threshold], color=:black, label="Best Threshold")
# plot!(xscale=:log10)
# xlabel!("X Threshold")
# ylabel!("BIC")

# p2 = histogram(
# 	x_samples, 
# 	normalize=true, 
# 	label="Samples";
# 	bins = 0:1:200 
# )
# vline!([threshold], color=:black, label="Best Threshold")

# plot(p1, p2)
```









```{julia}
#| label: fig-tail-fit
#| fig-cap: "A plot of the empirical survival function of the samples (blue) and the pareto distribution fit to the tail (red)."

# function fit_pareto_tail(x_tail_sorted::Vector{Float64})
# 	m = length(x_tail_sorted)
# 	θ = minimum(x_tail_sorted)
# 	α = 1.0 + m / (sum(log.(x_tail_sorted)) - m * log(θ))

# 	return Pareto(α, θ)
# end 

# function get_tail_index(x::Vector{Float64})
# 	x_sorted = sort(x, rev=true)
# 	m_max = trunc(Int, minimum([0.2 * length(x), 3.0 * sqrt(length(x))]))

# 	best_ks = Inf
# 	best_m = nothing
# 	for m in 2:m_max
# 		dist_i = fit_pareto_tail(x_sorted[1:m])
# 		ks_result = ExactOneSampleKSTest(x_sorted[1:m], dist_i)
# 		if ks_result.δ < best_ks
# 			best_ks = ks_result.δ
# 			best_m = m
# 		end
# 	end
# 	best_m
# end

# function tail_plot(x::Vector{Float64})
# 	x_sorted = sort(x)
# 	share_above = 1.0 .- (collect(1:length(x)) .- 1) / length(x)
# 	plot(x_sorted, share_above, xscale=:log10, yscale=:log10, seriestype=:line, label="Empirical")
# 	plot!(xlabel="X", ylabel="P(x > X)")
# end

# function tail_plot(x::Vector{Float64}, tail_dist::Pareto{Float64})
# 	tail_plot(x)
# 	α, θ = params(tail_dist)
	
# 	x_dist = sort(x[x .> θ])
# 	ks_result = ExactOneSampleKSTest(x_dist, tail_dist)
# 	y_dist = ccdf(tail_dist, x_dist) .* (length(x_dist) / length(x))
# 	plot!(x_dist, y_dist, label="Tail Pareto Fit")
# 	title!("KS Test Statistic: $(ks_result.δ)")
# end

# r = ordinalrank(x_samples, rev=true)
# m = get_tail_index(x_samples)

# x_tail = x_samples[r .<= m]
# tail_dist = fit_pareto_tail(x_tail)

# tail_plot(x_samples, tail_dist)
```

Now I will implement my method

```{julia}
#| label: fig-smoothed-data
#| fig-cap: "How the data points have changed after smoothing. Points that fall on the y=x line (black) are unchanged. The outliers in the smoothed distribution are still there, but now they are less extreme."

# function adjust_tail_value(x::Float64, inverse_rank_x::Int, tail_dist::Pareto{Float64}, max_tail_rank::Int)
# 	α, θ = params(tail_dist)
# 	if x <= θ
# 		return x
# 	else
# 		return quantile(tail_dist, (max_tail_rank - inverse_rank_x + 0.5) / max_tail_rank)
# 	end
# end

# function smooth_pareto_tail(x::Vector{Float64})
# 	r = ordinalrank(x, rev=true)
# 	n = length(x)
# 	m = get_tail_index(x)

# 	x_tail = x[r .<= m]
# 	tail_dist = fit_pareto_tail(x_tail)

# 	y_tail = pdf.(tail_dist, x)

# 	x_out = adjust_tail_value.(x, r, tail_dist, m)

# 	return x_out
# end

# x_samples_smoothed = smooth_pareto_tail(x_samples)
# x_range = [minimum(x_samples_smoothed); maximum(x_samples_smoothed)]

# l = @layout [a [b ; c]]

# h1 = histogram(
#      x_samples_smoothed, 
#      normalize=true, 
#      label="Smoothed X"
# )
# xlabel!("Smoothed X")

# h2 = histogram(
#      x_samples, 
#      normalize=true, 
#      label="Original X"
# )
# xlabel!("Original X")

# s1 = plot(
# 	x_samples, 
# 	x_samples_smoothed, 
# 	seriestype=:scatter,
# 	label="Data"
# )
# plot!(
# 	x_range, 
# 	x_range, 
# 	color=:black, 
# 	linestyle=:dash, 
# 	label="y=x"
# )
# plot!(xlabel="Original X", ylabel="Smoothed X")

# plot(s1, h1, h2, layout = l)
```

Now test it systematically

```{julia}
# function run_mean_estimation_simulation(n_iter::Int64, sample_size::Int64, true_dist::UnivariateDistribution, mean_estimator)
# 	tot_squared_error = 0.0
# 	tot_bias = 0.0
# 	n_covered = 0

# 	μ_true = mean(true_dist)
# 	for i in 1:n_iter
# 		μ, μ_lb, μ_ub = mean_estimator(rand(true_dist, sample_size))

# 		if (μ_true > μ_lb) & (μ_true < μ_ub)
# 			n_covered += 1
# 		end

# 		tot_squared_error += (μ - μ_true)^2
# 		tot_bias += μ - μ_true
# 	end

# 	rmse = sqrt(tot_squared_error / n_iter)
# 	bias = tot_bias / n_iter
# 	coverage = n_covered / n_iter

# 	return rmse, bias, coverage
# end

# function simple_mean_est(x::Vector{Float64})
# 	μ = mean(x)
# 	σ = std(x) / sqrt(length(x))
# 	μ_lb = μ - 1.645 * σ
# 	μ_ub = μ + 1.645 * σ

# 	return μ, μ_lb, μ_ub
# end

# run_mean_estimation_simulation(
# 	10000,
# 	10000, 
# 	data_dist, 
# 	simple_mean_est
# )
```

```{julia}
# function pareto_smoothed_mean_est(x::Vector{Float64})
# 	x_smooth = smooth_pareto_tail(x)
# 	return simple_mean_est(x_smooth)
# end

# run_mean_estimation_simulation(
# 	10000,
# 	10000, 
# 	data_dist, 
# 	pareto_smoothed_mean_est
# )
```

# OLD STUFF

In a lot of situations we have 1-D data which is skewed, and we want to remove the outliers on the right. This happens a lot when running experiments with continuous variables like revenue, where you can have a few data points which are very high revenue and hurt your variance a lot. Removing these outliers improves your variance a lot.

## Generate toy dataset

To test these methods, I need some synthetic data. I'll generate a mixture of a log-normal distribution and a Pareto distribution. The lognormal represents most users, but the Pareto represents the outliers. The pareto is especially problematic for statistics, since it has infinite variance, meaning the CLT won't work.

```{julia}
#| execute: false

# data_dist = MixtureModel(
# 	[
# 	   LogNormal(2.0, 0.5),
# 	   Pareto(1.8,15)
# 	], 
# 	[0.98, 0.02]
# );

# x_samples = rand(data_dist, 1000);

# x = LinRange(0.001,40,300)
# y = pdf.(data_dist, x)

# histogram(
#      x_samples, 
#      normalize=true, 
#      # bins = x, 
#      label="Samples" 
# )
# plot!(x,y,label="PDF", lw=3)
```

## Outlier Detection

Now we want to try to identify the outliers that came from the pareto distributon. I'll try to do this by modelling the data as:

1. A KDE density estimation. This is for the bulk data, and is non-parametric so it should work on any dataset. I need to choose a kernel, for this data I'll choose lognormal.
2. A generalized pareto distribution. This should model the outliers.

```{julia}
#| execute: false

# function kde(samples::Vector{Float64}, bandwidth::Float64, x::Float64, weights::Vector{Float64})
# 	sum(weights .* pdf.(Normal.(log.(samples), bandwidth), log(x))) / (sum(weights) * x)
# end

# function kde_loo(log_samples::Vector{Float64}, bandwidth::Float64, i::Int, weights::Vector{Float64})
# 	sum(weights[1:end .!= i] .* pdf.(Normal.(log_samples[1:end .!= i], bandwidth), log_samples[i])) / sum(weights)
# end

# function kde_logloss(
# 	samples::Vector{Float64},
# 	bandwidth::Float64,
# 	weights::Vector{Float64}
# )
# 	log_samples = log.(samples)
# 	kde_est = kde_loo.(Ref(log_samples), bandwidth, 1:length(samples), Ref(weights))
# 	return -1 * sum(weights .* log.(kde_est)) / sum(weights)
# end

# function fit_kde(samples::Vector{Float64}, weights::Vector{Float64})
# 	f(log_bw_i) = kde_logloss(samples, exp(log_bw_i[1]), weights);
# 	initial_x = [10.0];
# 	exp(Optim.minimizer(optimize(f, initial_x))[1])
# end

# function fit_kde(samples::Vector{Float64})
# 	fit_kde(samples, ones(length(samples)))
# end

# x_bandwidth = exp10.(LinRange(-2, 2, 100))
# y_bandwidth = kde_logloss.(Ref(x_samples), x_bandwidth, Ref(ones(length(x_samples))))
# plot(x_bandwidth, y_bandwidth, xscale=:log10)
```

```{julia}
#| execute: false

# bw = fit_kde(x_samples)
# y_kde = kde.(Ref(x_samples), bw, x, Ref(ones(length(x_samples))))

# histogram(
#      x_samples, 
#      normalize=true, 
#      bins = x, 
#      label="Samples" 
# )
# plot!(x,y,label="PDF", lw=3)
# plot!(x,y_kde,label="KDE", lw=3)
```


```{julia}
#| label: fig-kde
#| fig-cap: "To test my kde fitting algorithm, I put a random hard threshold at 200 and then fit the distribution. The KDE estimate looks like a very good approximation."

# mutable struct LooKDE
# 	x::Vector{Float64}
# 	log_x::Vector{Float64}
# 	w::Vector{Float64}
#     bw::Float64
# 	n::Int64
#     function LooKDE(x::Vector{Float64}, w::Vector{Float64})
# 		@assert length(x) == length(w)
# 		log_x = log.(x)
# 		n = length(x)
# 		bw = 1.06 * std(log_x, Weights(w)) * n^(-1/5.0)
#         return new(
# 			x,
# 			log_x,
# 			w,
# 			bw,
# 			n
# 		)
#     end 
# end

# function loo_pdf_logscale(kde::LooKDE, i::Int, bw::Float64)
# 	sum(kde.w[1:end .!= i] .* pdf.(Normal.(kde.log_x[1:end .!= i], bw), kde.log_x[i])) / sum(kde.w)
# end

# function loo_pdf_logscale(kde::LooKDE, i::Int)
# 	loo_pdf_logscale(kde, i, kde.bw)
# end

# function loo_pdf(kde::LooKDE, i::Int)
# 	loo_pdf_logscale(kde, i) / kde.x[i]
# end

# function loo_pdf(kde::LooKDE)
# 	[loo_pdf(kde, i) for i in 1:kde.n]
# end

# function loo_logloss(
# 	kde::LooKDE,
# 	bw::Float64
# )
# 	loo_pdf = [loo_pdf_logscale(kde, i, bw) for i in 1:kde.n]
# 	return -1 * sum(kde.w .* log.(loo_pdf)) / sum(kde.w)
# end

# function fit!(kde::LooKDE)
# 	f(log_bw_i) = loo_logloss(kde, exp(log_bw_i[1]))
# 	initial_x = [kde.bw]
# 	best_bw = exp(Optim.minimizer(optimize(f, initial_x))[1])
# 	kde.bw = best_bw
# end

# test_x = x_samples[x_samples .< 200.0]
# kde = LooKDE(test_x, ones(Float64, size(test_x)))
# fit!(kde)

# histogram(
#      test_x, 
#      normalize=true, 
#      label="Samples";
# 	 bins = 0:1:200 
# )

# idx_kde = sortperm(test_x)
# x_kde = test_x
# y_kde = loo_pdf(kde)
# plot!(x_kde[idx_kde], y_kde[idx_kde],label="KDE Estimate", lw=2)
# xlabel!("X")
```

```{julia}

# function init_p_tail(tail_index::Int64, x::Vector{Float64})
# 	w = zeros(Float64, size(x))
# 	w[1:tail_index] .= 0.5
# 	return w
# end

# function fit_pareto(log_x::Vector{Float64}, w::Vector{Float64})
# 	w_tot = sum(w)

# 	log_θ = minimum(log_x)
# 	α = w_tot / (sum(w .* log_x) - w_tot * log_θ)

# 	return Pareto(α, exp(log_θ))
# end

# mutable struct KDEParetoMixture
#     x::Vector{Float64}
# 	log_x::Vector{Float64}
#     p_tail::Vector{Float64}
# 	n::Int64
# 	kde::LooKDE
# 	tail_index::Int64
# 	pareto::Pareto
#     function KDEParetoMixture(x_in::Vector{Float64})
# 		x = sort(x_in, rev=true)
# 		log_x = log.(x)
# 		n = length(x)
# 		tail_index = set_tail_index(x)
# 		p_tail = init_p_tail(tail_index, x)
# 		pareto = fit_pareto(log_x[1:tail_index], p_tail[1:tail_index])
# 		kde = LooKDE(x, 1.0 .- p_tail)
# 		fit!(kde)

#         return new(
# 			x,
# 			log_x,
# 			p_tail,
# 			n,
# 			kde,
# 			tail_index,
# 			pareto
# 		)
#     end 
# end

# function step!(kde_mix::KDEParetoMixture)
# 	tot_p_tail = mean(kde_mix.p_tail)
# 	p_kde = loo_pdf(kde_mix.kde) .* (1.0 - tot_p_tail)
# 	p_pareto = pdf.(kde_mix.pareto, kde_mix.x) .* tot_p_tail
# 	p_tail = p_pareto ./ (p_kde .+ p_pareto)

# 	kde = LooKDE(kde_mix.x, 1.0 .- p_tail)
# 	fit!(kde)

# 	pareto = fit_pareto(kde_mix.log_x[1:kde_mix.tail_index], p_tail[1:kde_mix.tail_index])

# 	kde_mix.pareto = pareto
# 	kde_mix.kde = kde
# 	kde_mix.p_tail = p_tail
# end

# function plot_kde_mix(kde_mix)
# 	x_tail = kde_mix.x[1:kde_mix.tail_index]
# 	x_non_tail = kde_mix.x[(kde_mix.tail_index+1):length(kde_mix.x)]

# 	p1 = histogram(
# 		x_non_tail, 
# 		normalize=false, 
# 		label="Non Tail Samples";
# 		bins = 0:1:maximum(kde_mix.x),
# 		bar_width=1, 
# 		c=1, 
# 		lc=1
# 	)

# 	histogram!(
# 		x_tail, 
# 		normalize=false, 
# 		label="Tail Samples";
# 		bins = 0:1:maximum(kde_mix.x),
# 		bar_width=1, 
# 		c=2, 
# 		lc=2
# 	)
# 	xlabel!("X")

# 	p2 = plot(
# 		kde_mix.x,
# 		kde_mix.p_tail,
# 		label="Estimate"
# 	)
# 	true_pareto = pdf.(component(data_dist,3), kde_mix.x) .* probs(data_dist)[3]
# 	true_non_pareto = (
# 		pdf.(component(data_dist,1), kde_mix.x) .* probs(data_dist)[1] .+
# 		pdf.(component(data_dist,2), kde_mix.x) .* probs(data_dist)[2]
# 	)
# 	true_p_tail = true_pareto ./ (true_pareto .+ true_non_pareto)
# 	plot!(
# 		kde_mix.x,
# 		true_p_tail,
# 		label="True"
# 	)

# 	xlabel!("X")
# 	ylabel!("P(tail)")

# 	plot(p1, p2, layout=grid(2,1))
# end

# kde_mix = KDEParetoMixture(x_samples)
# step!(kde_mix)
# # step!(kde_mix)
# # step!(kde_mix)
# plot_kde_mix(kde_mix)
```