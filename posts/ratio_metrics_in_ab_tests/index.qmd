---
title: "The statistics of relative effect sizes in A/B testing."
date: "2023-06-25"
categories: [Python]
image: ../../_freeze/posts/models_vs_markets/index/figure-html/cell-2-output-1.png
jupyter: "python3"
description: "What's the best way to handle relative effects in A/B testing?"
draft: true
html-math-method: webtex
---

# Introduction

Ratio metrics are critical when analyzing online controlled trials.

# The likelihood ratio

The test statistic for the likelihood ratio test is: 

$$
\lambda_{\mathrm{LR}}=-2\left[l\left(\theta_0\right)-l(\hat{\theta})\right]
$$

Where $l\left(\theta\right)$  is the log-likelihood of the observed data, $\theta$ are the parameters of the model, $\theta_0$ is the parameters under the null hypothesis, and $\hat{\theta}$ are the maximum likelihood estimates of the parameters. The model for $\theta_0$ must be a subset of the model for $\hat{\theta}$, itâ€™s normally the same model but with 1 or more of the parameters fixed at a specific value.

The sampling distribution for $\lambda_{\mathrm{LR}}$ is often not know exactly, but if the sample size is large enough it should be approximately $\chi^2$ distributed, with the degrees of freedom equal to the difference in the number of free parameters in the two models. 

Let's see if we can use this to first derive the two sample welches t-test, which tests the absolute difference between the means of two samples. Then if we can do that, we can apply the same approach to create a test for the relative difference.

# The likelihood ratio for the means of two independent samples of data

Assume we have two samples of data, $\mathbf{x}$ and $\mathbf{y}$, with $n_x$ and $n_y$ samples respectively. I will model the two samples of data with two independent normal distributions. Therefore, I can write the joint likelihood of the two samples of data as:

$$
L(\mathbf{x}, \mathbf{y} | \theta) = \prod_{i=1}^{n_x} N(x_i | \mu_x, \sigma_x) \prod_{j=1}^{n_y} N(y_j | \mu_y, \sigma_y)
$$

Where $\theta = [\mu_x, \mu_y, \sigma_x, \sigma_y]$. And the log-likelihood as:

$$
l(\mathbf{x}, \mathbf{y} | \theta) = l(\theta) = \sum_{i=1}^{n_x} \ln \left( N(x_i | \mu_x, \sigma_x) \right) + \sum_{j=1}^{n_y} \ln \left( N(y_j | \mu_y, \sigma_y) \right)
$$

The log-likelihood of the normal distribution is:

$$
\ln \left( N(x | \mu, \sigma) \right) = - \frac{1}{2} \ln{2 \pi} - \ln{\sigma} - \frac{1}{2 \sigma^2}\left( x - \mu \right)^2
$$

Therefore, we can write the log-likelihood of our data as:

$$
l(\theta) = - \left( \frac{n_x + n_y}{2} \ln{2 \pi} + n_x \ln{\sigma_x} + n_y \ln{\sigma_y} + \frac{1}{2 \sigma_x^2} \sum_{i=1}^{n_x}  \left( x - \mu_x \right)^2 + \frac{1}{2 \sigma_y^2} \sum_{i=1}^{n_y}  \left( y - \mu_y \right)^2 \right)
$$

Now we don't care about the the standard deviations, our null hypotheses are always about the means. So let's maximise the log-likelihood with respect to $\sigma_x$ and $\sigma_y$.

$$
0 = \frac{\partial l}{\partial \sigma_x} = - \frac{n_x}{\sigma_x} + \frac{1}{\sigma_x^3} \sum_{i=1}^{n_x}  \left( x - \mu_x \right)^2
$$

Solving this gives (and the same for $\sigma_y$):

$$
\sigma_x^2 = \frac{1}{n_x} \sum_{i=1}^{n_x}  \left( x - \mu_x \right)^2
$$

Substituting this back into the likelihood function gives:

$$
l(\theta) = - \left( \frac{n_x + n_y}{2}  (\ln{2 \pi} + 1) + \frac{n_x}{2} \ln \left( \frac{1}{n_x} \sum_{i=1}^{n_x}  \left( x - \mu_x \right)^2 \right) + \frac{n_y}{2} \ln \left( \frac{1}{n_y} \sum_{i=1}^{n_y}  \left( y - \mu_y \right)^2 \right) \right)
$$

Now to estimate $l(\hat{\theta})$ I need to maximize with respect to $\mu_x$ and $\mu_y$ too. While I could do this again formally by differentiating $l(\theta)$, it's not really necessary. It's clear that the maximum is when $\mu_x = \bar{x}$, where $\bar{x}$ is the mean of $\mathbf{x}$, and the same for $y$. This gives:

$$
l(\hat{\theta}) = - \left( \frac{n_x + n_y}{2}  (\ln{2 \pi} + 1) + \frac{n_x}{2} \ln \hat{\sigma}_x^2 + \frac{n_y}{2} \ln \hat{\sigma}_y^2 \right)
$$

Where $\hat{\sigma}_x$ and $\hat{\sigma}_y$ are the uncorrected sample standard deviations of $\mathbf{x}$ and $\mathbf{y}$. Now using these two expressions, I can calculate $\lambda_{\mathrm{LR}}$ for the general case under the null hypothesis that the population mean of $\mathbf{x} = \mu_x$ and the population mean of $\mathbf{y} = \mu_y$:

$$
\lambda_{\mathrm{LR}}(\mu_x, \mu_y) = n_x \ln \left( \frac{1}{n_x \hat{\sigma}_x^2} \sum_{i=1}^{n_x}  \left( x - \mu_x \right)^2 \right) + n_y \ln \left( \frac{1}{n_y \hat{\sigma}_y^2} \sum_{i=1}^{n_y}  \left( y - \mu_y \right)^2 \right)
$$

Now I can use a trick to simplify this further, by noting that:

\begin{align}
\frac{1}{n_x \hat{\sigma}_x^2} \sum_{i=1}^{n_x}  \left( x - \mu_x \right)^2 &= \frac{1}{n_x \hat{\sigma}_x^2} \sum_{i=1}^{n_x}  \left( x - \bar{x} + \bar{x} - \mu_x \right)^2 \nonumber \\
&= \frac{1}{n_x \hat{\sigma}_x^2} \sum_{i=1}^{n_x}  \left( \left( x - \bar{x} \right)^2 + \left( \bar{x} - \mu_x \right)^2 + 2 (x - \bar{x})(\bar{x} - \mu_x) \right) \nonumber \\
&= \frac{1}{n_x \hat{\sigma}_x^2} \sum_{i=1}^{n_x}  \left( \left( x - \bar{x} \right)^2 + \left( \bar{x} - \mu_x \right)^2 \right) \nonumber \\
&= 1 + \frac{\left( \bar{x} - \mu_x \right)^2}{\hat{\sigma}_x^2} \nonumber
\end{align}

Finally substituting that back into the original equation gives:

$$
\lambda_{\mathrm{LR}}(\mu_x, \mu_y) =n_x \ln \left( 1 + \frac{(\bar{x} - \mu_x)^2}{\hat{\sigma}_x^2} \right) + n_y \ln \left( 1 + \frac{(\bar{y} - \mu_y)^2}{\hat{\sigma}_y^2} \right)
$$

This can be approximated using the taylor expansion:

$$
\ln(1+x^2) \approx x^2 + O(x^4)
$$

Using this gives:

$$
\lambda_{\mathrm{LR}}(\mu_x, \mu_y) =\frac{(\bar{x} - \mu_x)^2}{\hat{\sigma}_x^2 / n_x} + \frac{(\bar{y} - \mu_y)^2}{\hat{\sigma}_y^2 / n_y}
$$

This looks like the sum of two t-distributed random variables squared. I will simplify this further by defining the standard error as, giving:

$$
\lambda_{\mathrm{LR}}(\mu_x, \mu_y) = \frac{(\bar{x} - \mu_x)^2}{\sigma^2_{\mu, x}}
$$

```{python}
# \lambda_{\mathrm{LR}}(\mu_x, \mu_y) = \frac{(\bar{x} - \mu_x)^2}{\sigma_\bar{x}^2} + \frac{(\bar{y} - \mu_y)^2}{\sigma_\bar{y}^2}
```

# Deriving Welch's t-test from the likelihood ratio

The Welch's t-test tests the null hypothesis that the absolute difference between two means is some fixed value:

\begin{align}
H_0 &: \quad \mu_y - \mu_x = \Delta \nonumber \\
H_A &: \quad \mu_y - \mu_x \neq \Delta\nonumber
\end{align}

To get the test statistic for this, I need to minimize $\lambda_{\mathrm{LR}}$ under the constraint that $\mu_y - \mu_x = \Delta$. To do this I'll substitute $\mu_y = \mu_x + \Delta$ into the equation for $\lambda_{\mathrm{LR}}$.

\begin{align}
\lambda_{\mathrm{LR}}(\mu_x, \Delta) &= \frac{(\bar{x} - \mu_x)^2}{\sigma_\bar{x}^2}+ \frac{(\bar{y} - \mu_x - \Delta )^2}{\sigma_\bar{y}^2} \nonumber \\
&= \frac{(\mu_x - \bar{x})^2}{\sigma_\bar{x}^2}+ \frac{(\mu_x - (\bar{y} - \Delta) )^2}{\sigma_\bar{y}^2} \nonumber
\end{align}

Now it can be shown that in general:

$$
\min_x \left\{ \frac{(x - b)^2}{a^2} + \frac{(x - d)^2}{c^2} \right\} = \frac{(b - d)^2}{a^2 + c^2}
$$

I can use this to find the minimum of $\lambda_{\mathrm{LR}}$ without doing lots of differentiation. It gives:

$$
\lambda_{\mathrm{LR}}(\Delta) = \frac{\left( \bar{y} - \bar{x} - \Delta \right)^2}{\sigma_\bar{x}^2 + \sigma_\bar{y}^2}
$$

Finally, taking the square root of this, gives the Welch's t-test statistic:

$$
t
$$

```{python}
# t_\Delta(\Delta) = \frac{\bar{y} - \bar{x} - \Delta}{\sqrt{\sigma_\bar{x}^2 + \sigma_\bar{y}^2}}
```

So we have successfully derived the Welch's t-test from the likelihood ratio!

# Deriving a statistical test for the ratio of means

Now, given that this works for the regular difference between two means, let's see what we get for a ratio! This is a different null hypothesis:

\begin{align}
H_0 &: \quad \frac{\mu_y}{\mu_x} = \delta \nonumber \\
H_A &: \quad  \frac{\mu_y}{\mu_x} \neq \delta \nonumber
\end{align}

Taking the same approach as before, I substitute in that $\mu_y = \delta \mu_x$:

\begin{align}
\lambda_{\mathrm{LR}}(\mu_x, \delta) &= \frac{(\bar{x} - \mu_x)^2}{\sigma_\bar{x}^2}+ \frac{(\bar{y} - \mu_x \delta )^2}{\sigma_\bar{y}^2} \nonumber \\
&= \frac{(\mu_x - \bar{x})^2}{\sigma_\bar{x}^2}+ \frac{(\mu_x - \frac{\bar{y}}{\delta}) )^2}{\sigma_\bar{y}^2 / \delta^2} \nonumber \nonumber
\end{align}

And using the same trick again I can find the minimum:

$$
\lambda_{\mathrm{LR}}(\delta) = \frac{\left( \bar{y} - \delta \bar{x} \right)^2}{\sigma_\bar{y}^2 + \delta^2 \sigma_\bar{x}^2 }
$$

And again taking the square root gives a t-distributed metric:

$$
t_\delta(\delta) = \frac{\bar{y} - \delta \bar{x} }{\sqrt{\sigma_\bar{y}^2 + \delta^2 \sigma_\bar{x}^2}}
$$

This metric can be used as a statistical test for the ratio $\delta$!

## Comparison with Welch's t-test

Now while this is t-distributed, it's actually different to the original Welch's t-statistic. I can check that by substituting the absolute difference in for the relative difference, $\delta = \frac{\Delta}{\bar{x}} + 1$. This gives:

$$
t_\delta(\Delta) = \frac{ \bar{y} -\bar{x} - \Delta }{\sqrt{\sigma_\bar{y}^2 + \left(\frac{\Delta}{\bar{x}} + 1 \right)^2 \sigma_\bar{x}^2 }} \neq t_\Delta(\Delta)
$$

Interestingly the numerator is the same as in $t_\Delta$, but the denominator is different! The variance depends on $\Delta$. So we've found something new, it's not just rearranging Welch's t-test. 

Now while these are not equal in general, they are equal in the very important case - the case where you are testing the null hypothesis that $\mu_x = \mu_y$, giving $\Delta = 0$ and $\delta = 1$. In this case, the above formula reduces to:

$$
t_\delta(0) = \frac{ \bar{y} -\bar{x}}{\sqrt{\sigma_\bar{y}^2 +  \sigma_\bar{x}^2 }} = t_\Delta(0)
$$

This is a very useful finding. If this were not the case, you could have different results for whether you can reject the null hypothesis that two samples have the same population mean, just depending on whether you care about reporting on the relative or absolute difference. It's of course sensible that the statistical tests should be different in general, but in this special case it's sensible that they coincide.

# Confidence intervals

We have a statistical test now, but what about confidence intervals? Cox and Hinkly have a very nice definition of the confidence interval in their book, Theoretical Statistics. They say that the confidence interval contains all the values that are statistically consistent with the data.

So we can construct confidence intervals in the same way. They are all the values that the null hypothesis that the population has that value would not be rejected by our new statistical tests.

For a specific false positive rate, $\alpha$, we reject the values that are more extreme than the critical t-values, $\pm t_{1 - \frac{\alpha}{2}}$. Therefore, we have that:

\begin{align}
\mp t_{1 - \frac{\alpha}{2}} &= t_\Delta(\Delta_{\pm}) \nonumber \\
\mp t_{1 - \frac{\alpha}{2}} &= t_\delta(\delta_{\pm}) \nonumber
\end{align}

Note that, above, the signs are opposite. This is because the highest value of $\Delta$ or $\delta$ causes the statistic to be lower. 

## Confidence interval for the absolute difference

First the confidence interval for the absolute difference, $\Delta_\pm$. 

$$
\mp t_{1 - \frac{\alpha}{2}} = \frac{\bar{y} - \bar{x} - \Delta_{\pm}}{\sqrt{\sigma_\bar{x}^2 + \sigma_\bar{y}^2}}
$$

Re-arranging that gives the expected t-test confidence interval:

$$
\Delta_{\pm} =\bar{y} - \bar{x} \pm t_{1-\frac{\alpha}{2}} \sqrt{\sigma_\bar{x}^2 + \sigma_\bar{y}^2}
$$

## Confidence interval for the relative difference

Now doing the same thing for the relative difference. We start with:

$$
\mp t_{1 - \frac{\alpha}{2}} = \frac{\bar{y} - \delta_\pm \bar{x} }{\sqrt{\sigma_\bar{y}^2 + \delta_\pm^2 \sigma_\bar{x}^2}}
$$

This is slightly trickier to re-arrange, but I'll give it a go:

\begin{align}
t_{1 - \frac{\alpha}{2}}^2 &= \frac{(\bar{y} - \delta_\pm \bar{x})^2}{\sigma_\bar{y}^2 + \delta_\pm^2 \sigma_\bar{x}^2} \nonumber \\
0 &= (\bar{y} - \delta_\pm \bar{x})^2 - t_{1 - \frac{\alpha}{2}}^2 \left( \sigma_\bar{y}^2 + \delta_\pm^2 \sigma_\bar{x}^2 \right) \nonumber \\ 
0 &= \bar{y}^2 + \delta_\pm^2 \bar{x}^2 - 2 \delta_\pm \bar{y} \bar{x} - t_{1 - \frac{\alpha}{2}}^2 \sigma_\bar{y}^2 - t_{1 - \frac{\alpha}{2}}^2 \delta_\pm^2 \sigma_\bar{x}^2 \nonumber \\ 
0 &= \delta_\pm^2 ( \bar{x}^2 - \sigma_\bar{x}^2 t_{1 - \frac{\alpha}{2}}^2) - 2 \delta_\pm \bar{y} \bar{x} + \bar{y}^2 - t_{1 - \frac{\alpha}{2}}^2 \sigma_\bar{y}^2  \nonumber
\end{align}

Now I can solve this quadratic equation, giving:

$$
\delta_\pm = \frac{\bar{y} \bar{x} \pm \sqrt{\bar{y}^2 \bar{x}^2 - ( \bar{x}^2 - \sigma_\bar{x}^2 t_{1 - \frac{\alpha}{2}}^2) (\bar{y}^2 - t_{1 - \frac{\alpha}{2}}^2 \sigma_\bar{y}^2)}}{\bar{x}^2 - \sigma_\bar{x}^2 t_{1 - \frac{\alpha}{2}}^2}
$$

This is the same as the confidence interval from Fieller's method!

# Conclusion

This note explored the relationship between the t-test, the likelihood ratio test and Fieller's confidence interval. I showed that Fieller's confidence interval for a ratio metric can be derived via the likelihood ratio test, in the same way that Welch's t-test arises for the absolute difference.

These findings lead me to recommend the following approach when analysing A/B test results:

1. Use Welch's t-test to calculate a p-value, and if the p-value < $\alpha$, reject the null hypothesis that A and B are equal.
2. When reporting on the absolute difference, use the standard t-test confidence interval.
3. When reporting on the relative difference, use Fieller's method to calculate the confidence interval.

The nice thing about this approach is that all three results will always be aligned. If the p-value does not reject the null hypothesis, then both confidence intervals will include 0, while if the p-value does reject the null hypothesis then neither confidence interval will include 0.

```{python}
import numpy as np
```

